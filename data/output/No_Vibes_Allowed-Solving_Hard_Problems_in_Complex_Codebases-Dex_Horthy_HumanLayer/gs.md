# 无“幻觉”编程：在复杂代码库中解决难题 - 完整中文逐字稿

## 基本信息
- **视频时长**：[20:31]
- **讲者数量**：1位 (Dex Horthy)
- **转录时间**：2023年10月
- **核心主题**：AI 编程智能体（Agents）、上下文工程（Context Engineering）、复杂代码库维护

## 完整逐字稿

### [00:00] Dex Horthy
大家好！大家感觉怎么样？太令人兴奋了。我是 Dex。正如他们在精彩的介绍中所说，我研究智能体（Agents）已经有一段时间了。我们在 6 月份的 AI Engineer 大会上发表的关于“12 要素智能体”的演讲，是有史以来最受欢迎的演讲之一——我想大概是前八名吧，是那次大会最好的演讲之一。当时我可能提到了关于“上下文工程”（Context Engineering）的内容。

那我今天为什么站在这里？我要讲些什么？我想谈谈 6 月份 AI Engineer 大会上我最喜欢的一个演讲。我知道大家昨天都听到了 Igor 的更新，但他们不让我改 PPT 了，所以这里还是要讲 Igor 在 6 月份谈到的内容。

### [00:53] Dex Horthy
基本上，他们调查了不同规模公司的 10 万名开发者，发现大多数时候在软件工程中使用 AI，都会导致大量的返工（rework）和代码库的混乱。而且它在处理复杂任务或遗留代码库（Brownfield Codebases）时表现并不好。

## 🔍 图像补充说明
- **[01:08] 画面内容**：屏幕展示了一张柱状图，标题为“在软件工程中使用 AI 增加了返工量”。红框高亮显示了“Reworked”（返工）部分显著增加，表明虽然产出总量增加了，但很多都是低质量代码。
- **[01:15] 画面内容**：展示了一个 2x2 矩阵图。
    - **左下角（低复杂度/新项目）**：生产力提升 +35-40%。
    - **右上角（高复杂度/遗留项目）**：生产力提升仅 +0-10%。说明 AI 在复杂的老旧系统中效果很差。

### [01:29] Dex Horthy
你可以从图表中看到，虽然你的发布量大大增加了，但很多都只是在重做你上周发布的那些“垃圾代码”（Slop）。另一方面是，如果你是在做全新的项目（Greenfield），比如 Vercel 的仪表盘之类的，效果会很好。但如果你要去处理一个有 10 年历史的 Java 代码库，效果就不那么理想了。

这与我的个人经验非常吻合。我和许多聪明的创始人及优秀的工程师交流过，大家都说：“垃圾代码太多了”、“简直是技术债务工厂”、“它在大型代码库中根本无法工作”。甚至有人说：“也许等到模型变得更好的那一天吧。”

### [01:39] Dex Horthy
但这正是**上下文工程**（Context Engineering）的核心所在。上下文工程意味着如何从**今天**的模型中挖掘最大价值。我们要如何管理我们的上下文窗口？我们在 8 月份讨论过这个问题。我得承认一件事：第一次使用 Claude Code 时，我并不觉得有什么了不起。我觉得，好吧，是好了一点，我喜欢它的用户体验。

但从那以后，我们要作为一个团队解决这个问题。我们实际上能够获得 2 到 3 倍的吞吐量。我们的发布量如此之大，以至于我们别无选择，只能改变我们的协作方式。我们彻底重新构建了我们开发软件、沟通和审查的方式。这对于我们这个三人团队来说，花了 8 周时间，真的非常非常难。但现在我们解决了这个问题，我们再也回不去了。这就是所谓“拒绝垃圾代码”的核心。

### [02:27] Dex Horthy
我们在 9 月份在这个话题上有点病毒式传播。我们的目标——这也是我们反向推导出来的——我们需要 AI 能够在遗留代码库中良好运作，解决复杂问题，不再产生垃圾代码，并且我们必须保持**认知对齐**（Mental Alignment）。稍后我会详细解释这意味着什么。当然，我们要尽可能利用好每一个 Token。

### [02:52] Dex Horthy
这就是针对编程智能体的高级上下文工程。

## 🔍 图像补充说明
- **[03:01] 画面内容**：展示了“最天真”的使用方式图解。即一直在这个对话框里对话，直到上下文耗尽。
- **[03:10] 画面内容**：展示了“稍微聪明一点”的方式。当对话偏离正轨时，重新开启一个对话窗口（Refresh Context），用之前的总结作为新提示词。

### [03:00] Dex Horthy
我想先定个调：使用编程智能体最天真的方式，就是向它索取东西，告诉它为什么错了，重新引导它，一直问啊问，直到你的上下文用完，或者你放弃，或者你崩溃大哭。

稍微聪明一点的做法是——大多数人很早就会发现这一点——如果你发现对话跑偏了，最好是重新开始一个新的上下文窗口。你说：“好吧，那条路走不通，我们重新开始。”同样的提示词，同样的任务，但这次我们要换条路走。你怎么知道什么时候该重开呢？如果你看到这种回复...

## 🔍 图像补充说明
- **[03:36] 画面内容**：屏幕上显示一行巨大的字：“You're absolutely right.”（你说得完全正确）。这是 AI 开始胡言乱语并试图讨好人类时的典型特征。

### [03:39] Dex Horthy
（观众笑）
如果你看到这句话，大概就是时候重开了。当 Claude 开始说你完全正确的时候，通常意味着它搞砸了。

我们可以做得更聪明，我称之为**“有意压缩”**（Intentional Compaction）。这基本上意味着，无论你是否在正轨上，你都可以利用现有的上下文窗口，让智能体将其压缩成一个 Markdown 文件。你可以审查它，标记它。然后当新的智能体启动时，它可以直接进入工作状态，而不必再做所有的搜索和代码库理解工作。

### [04:06] Dex Horthy
我们在压缩什么？问题在于，是什么占据了上下文窗口的空间？是查找文件、理解代码流、编辑文件、测试和构建输出。如果你用的是那种会把 JSON 和一大堆 UUID 倾倒进上下文窗口的 MCP（模型上下文协议）工具，那真是上帝保佑你。

那么我们应该压缩什么？具体的我会稍后讲，但这是一个很好的压缩示例。这包含了我们正在处理的具体内容、确切的文件和行号。

### [04:36] Dex Horthy
为什么我们要如此痴迷于上下文？因为大语言模型（LLM）是... 我其实在 YouTube 上因为说它们是纯函数而被喷了，因为它们是不确定的，但它们确实是**无状态**（Stateless）的。要从 LLM 获得更好表现的唯一方法，就是输入更好的 Token。

在循环的每一步，当 Claude 或任何编程智能体选择下一个工具时——可能会有数百个正确的下一步和数百个错误的下一步——唯一影响接下来输出内容的，就是**目前的对话内容**。

### [05:05] Dex Horthy
所以我们要优化这个上下文窗口，关注四个方面：正确性、完整性、大小，还有一点**轨迹**（Trajectory）。轨迹这点很有趣。很多人说：“我告诉智能体做某事，它做错了。所以我纠正了它，吼了它。然后它又做错了，我又吼了它。”

LLM 看着这段对话会想：“好的，酷。我做错事，人类吼我；我做错事，人类吼我。所以这段对话中下一个最可能的 Token 是：我最好再做错点什么，好让人类再吼我一次。”

所以要注意你的轨迹。

### [05:47] Dex Horthy
关于上下文大小，Jeff Huntley 做过很多关于编程智能体的研究，他说得很好：**你使用上下文窗口越多，你得到的结果就越差。**

这就引出了一个非常“学术”的概念，叫做**“愚笨区”**（The Dumb Zone）。

## 🔍 图像补充说明
- **[06:00] 画面内容**：图表展示了一个 168k Token 的上下文窗口。
- **[06:06] 视觉补充**：一条线画在 **40%** 的位置。
- **核心观点**：横线以上是“智能区”（Smart Zone），横线以下（使用超过40%后）是“愚笨区”（Dumb Zone）。一旦超过这个阈值，模型的逻辑推理能力会显著下降。

### [06:00] Dex Horthy
你有大约 16.8 万 Token 的窗口。大约在 **40%** 的线附近，你会开始看到收益递减，这取决于你的任务。如果你在编程智能体中有太多的 MCP 工具，你就是在“愚笨区”里工作，你永远不会得到好结果。40% 是一个很好的指导线。

### [06:29] Dex Horthy
回到压缩，或者我现在称之为“巧妙地避开愚笨区”。我们可以使用**子智能体**（Sub-agents）。如果你有一个前端子智能体、后端子智能体、QA 子智能体... 请停下来。子智能体不是用来搞角色扮演游戏的，它们是用来**控制上下文**的。

所以你可以做的是，比如你想在一个大型代码库中找到某个东西是如何工作的，你可以让编程智能体分叉（fork）出一个新的上下文窗口。这个新窗口负责所有的阅读、搜索、查找，阅读整个文件以理解代码库。然后，只返回一条非常、非常简洁的信息给父智能体。

## 🔍 图像补充说明
- **[07:15] 画面内容**：图示展示了父智能体不需要读取所有文件，而是指派一个子智能体去“查找 XYZ 在哪里处理”。子智能体完成繁重工作后，仅返回“文件在 src/main/...”这一行关键信息给父智能体。

### [07:29] Dex Horthy
这非常强大。比子智能体更好的是一种我称之为**“频繁有意压缩”**（Frequent Intentional Compaction）的工作流。我们要围绕上下文管理构建整个工作流。

这分为三个阶段：**研究（Research）、计划（Plan）、实施（Implement）**。我们的目标是全程保持在“智能区”。

### [07:51] Dex Horthy
**研究阶段**是为了理解系统如何工作，找到正确的文件，保持客观。
**计划阶段**你要列出确切的实施步骤，包括文件名和代码片段，并且要非常明确每次变更后如何测试。
然后是**实施阶段**。如果你读过这些计划，你会发现即使是世界上最笨的模型也不太可能搞砸。我们就按计划执行，保持低上下文占用。

### [08:30] Dex Horthy
我想把这付诸实践。我和我的朋友 Vaibhav（BoundaryML 的 CEO）录播客时，我说我要尝试一次性解决你们那个 30 万行 Rust 代码库中的一个问题。我们在周六坐了 7 个小时，向 Baml 发布了 3.5 万行代码。Vaibhav 估计这通常需要 1 到 2 周的工作量，我们在 7 小时内完成了。

**确认：它在遗留代码库中有效，且没有垃圾代码。**

### [09:37] Dex Horthy
但这也有局限性。我和我的朋友 Blake 试着从 Parquet Java 中移除 Hadoop 依赖。那次**非常不顺利**。如果你知道 Parquet Java 是什么，我对你的遭遇表示同情。那个依赖关系图太复杂了。在某一点上，我们不得不把所有东西都扔掉，回到白板上。

这就引出了 Jake 将要谈到的一个有趣观点：**不要外包思考**。AI 不能替代思考，它只能放大你已经完成的思考，或者放大你的缺乏思考。

### [10:19] Dex Horthy
人们问：“Dex，这就是**规格驱动开发**（Spec-Driven Development, SDD）吗？”
**不。** “规格驱动开发”这个词已经坏掉了。不是理念坏了，是这个词坏了。
Martin Fowler 在 2006 年说过“语义扩散”（Semantic Diffusion）。我们想出一个好词，有个好定义，然后每个人都开始兴奋，每个人都开始用它指代 100 种不同的东西，然后它就没用了。

就像“Agent”这个词一样。Agent 是人、是微服务、是聊天机器人... 现在 Agent 只是“循环中的工具”。SDD 也是一样，人们现在认为写个更好的 Prompt 就是 SDD。或者仅仅是在写代码时用一堆 Markdown 文件。

### [11:50] Dex Horthy
所以我不想用那个词。我想谈谈实际上有效的四个战术步骤：

**1. 研究 = 压缩 (Research == Compression)**
理解系统如何工作。如果你不给智能体做“入职培训”（Onboarding），它们就会开始编造东西。
就像电影《记忆碎片》（Memento）里的男主角，醒来没有记忆，必须读纹身才知道自己是谁。LLM 也是一样。

### [12:25] Dex Horthy
如果你的代码库很大，你可以做**“分层切分”**（Sharding down the stack）或**“渐进式披露”**（Progressive Disclosure）。与其把所有文档塞进去，不如只在仓库根目录放一个文件，然后在每一层级放额外的上下文。

但问题是文档会过时。这就引出了图表：代码库中的谎言数量随时间增加。所以与其依赖文档，不如使用**按需压缩上下文**。
如果我要开发一个新功能，我会告诉智能体：“去把这部分代码库的垂直切片（Vertical Slices）研究一遍，然后生成一份`research.md`。” 这是一份仅针对当前任务的、基于真实代码的压缩文档。

### [14:41] Dex Horthy
**2. 计划 = 杠杆 (Planning == Leverage)**
计划是对意图的压缩。在计划中列出确切步骤。
这里我想停下来谈谈**认知对齐**（Mental Alignment）。代码审查（Code Review）是为了什么？不仅仅是找 Bug，最重要的是让团队对代码库的变化保持认知一致。

我可以每周读几百行代码的计划书，但我不想读几千行 Go 语言的代码。如果计划写得好，那就足够让我保持对系统的理解，并在早期发现问题。

### [15:35] Dex Horthy
Mitchell Hashimoto 发过一个推文，说他把所有的思考过程和 Prompt 都在 PR（Pull Request）里分享出来。这就让审查者经历了一次旅程，而不仅仅是看最后的绿色代码墙。

### [16:01] Dex Horthy
**3. 你的目标是杠杆率**
你需要模型做正确的事情。这里有一个图表：随着计划变得越长，可读性下降，可靠性也会在某个点之后下降。你需要找到那个甜蜜点（Sweet Spot）。

### [16:39] Dex Horthy
**4. 不要外包思考**
这不是魔法。如果你不读计划，这就行不通。如果你需要同行评审，就把计划发给别人看。

如果你能从这个演讲中带走一件事，那就是：
- 一行糟糕的代码 = 一行糟糕的代码。
- 一个糟糕的计划 = 100 行糟糕的代码。
- **一行糟糕的研究（误解了系统） = 1000+ 行糟糕的代码。**

你要把人类的精力和注意力转移到这个管道中**杠杆率最高**的部分（即研究和计划阶段）。

### [17:46] Dex Horthy
有时候这是杀鸡用牛刀。如果你只是改个按钮颜色，直接告诉智能体就行了。随着问题难度的增加，你需要做的上下文工程和压缩工作就越多。这需要练习（Reps）。你没有银弹，也没有完美的 Prompt。

### [18:57] Dex Horthy
如果你想要一个看起来很厉害的词，你可以叫它“线束工程”（Harness Engineering），这是上下文工程的一部分。

**下一步是什么？**
我认为编程智能体将会商品化。真正的难点在于**团队和工作流的转型**。如果不解决这个问题，你就完了。
现在有一个鸿沟：高级工程师不爱用 AI，因为清理垃圾代码很烦；初级工程师用得很欢。这不是 AI 的错。如果要成功，需要自上而下的文化变革。

如果你是技术领导者，选**一个**工具，多加练习。

### [19:54] Dex Horthy
如果你想帮忙，我们正在招聘。我们正在构建一个智能体 IDE，帮助各种规模的团队加速迈向 99% AI 生成代码的旅程。
谢谢大家的能量！

---

## 🔍 图像补充说明

- **[12:00] 视觉补充**：幻灯片上展示了电影《记忆碎片》（Memento）的海报。讲者用这部电影比喻 LLM：每次交互就像主角醒来一样，没有任何记忆，必须依靠“纹身”（即我们提供的上下文）来了解现状。
- **[13:46] 画面内容**：展示了一个手绘图表，Y 轴标注为“谎言的数量”（Amount of lies）。随着时间推移，文档（Documentation）中的谎言数量激增，而代码（Actual Code）始终是真实的。这强调了为何静态文档不可靠，必须依靠实时生成的代码分析。
- **[15:03] 画面内容**：代码审查的需求金字塔。
    - **底层（最重要）**：认知对齐（Mental Alignment）。
    - **中层**：正确的解决方案、设计讨论。
    - **顶层（最不重要）**：风格（Style）。
    - *核心观点*：AI 时代，Code Review 的核心价值回归到底层的“认知对齐”。
- **[17:30] 画面内容**：杠杆率层级图。
    - 1 行错误代码 = 1 行错误代码的代价。
    - 1 行错误计划 = 10-100 行错误代码。
    - 1 行错误研究（误解系统） = 1000+ 行错误代码。
    - 1 行错误规格 = 10000+ 行错误代码。
    - *结论*：越往上游（研究/规格）投入人类精力，回报越高。

## 📚 重要术语和人物

- **Context Engineering (上下文工程)**：一种通过优化输入给大语言模型的信息（上下文），以获得最佳输出结果的技术。
- **The Dumb Zone (愚笨区)**：当 LLM 的上下文窗口使用率超过一定阈值（通常约为 40%）后，其逻辑推理和指令遵循能力显著下降的区域。
- **Context Compaction (上下文压缩)**：通过总结、提取关键信息或生成中间文件（如 `research.md`），减少占用上下文窗口的技术，旨在保持模型在“智能区”工作。
- **Brownfield Codebase (遗留代码库)**：指已经存在的、通常较为复杂且带有技术债务的老旧代码库。
- **Slop (垃圾代码)**：特指 AI 生成的、虽然能运行但质量低下、难以维护或充满冗余的代码。
- **Mental Alignment (认知对齐)**：团队成员之间对于系统架构、代码变更意图和实现路径的理解保持一致。
- **RPI Framework (Research/Plan/Implement)**：讲者提倡的一种 AI 编程工作流，即先研究代码库生成文档，再制定详细计划，最后才进行代码实施。

## 💡 我的学习收获

### 我学到的核心内容
我从这个视频中学到了 AI 编程不仅仅是写 Prompt，而是关于**上下文管理**的艺术。核心在于避免让 AI 进入“愚笨区”（超过 40% 的上下文占用）。最有效的方法是将开发过程拆解为“研究、计划、实施”三个阶段，并在每个阶段通过生成中间文档（如 markdown 文件）来“压缩”信息，确保 AI 始终能在高质量的上下文中工作。

### 我的理解和思考
我认为最重要的观点是**“不要外包思考”**（Do Not Outsource The Thinking）。AI 极大地提高了代码产出的效率，但如果人类不在最上游（研究和计划阶段）进行把关，生成的将只是“垃圾代码”（Slop）。未来的高级工程师的核心竞争力，将从“写代码”转变为“设计上下文”和“审查计划”，以确保团队的认知对齐。

### 我的实践计划
我计划将“研究-计划-实施”的工作流应用到我的日常开发中。具体做法是：在开始复杂任务前，先让 AI 阅读相关代码并生成一份 `research.md` 总结现状，然后让它基于此生成一份包含具体步骤和测试方法的 `plan.md`。只有在我作为人类审查并通过了这个计划后，才让 AI 开始写代码。

### 我想进一步探索的
我希望深入了解如何构建自动化的“子智能体”来执行特定的研究任务（例如“查找所有用到 X 库的地方”），以及如何将这种工作流更好地集成到现有的 IDE（如 VS Code 或 Cursor）中，而不是手动复制粘贴 markdown 文件。

## ⏰ 关键时间节点
- [01:42] 上下文工程的定义
- [06:00] “愚笨区”理论（40% 上下文阈值）
- [07:33] 研究/计划/实施（RPI）工作流详解
- [15:03] 代码审查的新核心：认知对齐
- [17:30] 错误的杠杆率层级（为什么研究比写代码更重要）