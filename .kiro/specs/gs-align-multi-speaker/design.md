# Design Document: GS-Align Multi-Speaker

## Overview

This design describes an improved alignment system that generates complete, high-quality `audio.srt` files from `gs.md` reference documents with full multi-speaker support. The system addresses the current limitation where only 240 of 414 entries are generated due to incomplete gs.md coverage and lack of speaker tracking.

## Architecture

```mermaid
flowchart TD
    subgraph Input
        GS[gs.md]
        SRT[Original SRT]
        GLOSS[glossary.yaml]
        VOICE[voice_map.json]
    end
    
    subgraph Parser
        GSP[GS Parser]
        SRTP[SRT Parser]
    end
    
    subgraph Core
        ALIGN[Alignment Engine]
        SPEAKER[Speaker Tracker]
        SPLIT[Text Splitter]
    end
    
    subgraph Output
        AUDIO[audio.srt]
        REPORT[Coverage Report]
    end
    
    GS --> GSP
    SRT --> SRTP
    GSP --> ALIGN
    SRTP --> ALIGN
    GLOSS --> ALIGN
    GSP --> SPEAKER
    SPEAKER --> ALIGN
    ALIGN --> SPLIT
    SPLIT --> AUDIO
    ALIGN --> REPORT
    VOICE --> SPEAKER
```

## Components and Interfaces

### 1. GS Parser (`GSParser`) - è¯­ä¹‰è§£æ

ä½¿ç”¨ LLM è¿›è¡Œè¯­ä¹‰è§£æ gs.md æ–‡ä»¶ï¼Œè€Œéç®€å•çš„æ­£åˆ™åŒ¹é…ã€‚è¿™æ ·å¯ä»¥æ›´å‡†ç¡®åœ°ç†è§£æ–‡æ¡£ç»“æ„å’Œå†…å®¹ã€‚

```python
@dataclass
class GSSegment:
    start_ms: int
    speaker: str
    text: str
    section_type: str  # 'transcript', 'glossary', 'notes', 'learning'
    
class GSParser:
    def __init__(self, use_llm: bool = True):
        self.use_llm = use_llm
        
    def parse(self, gs_path: Path) -> List[GSSegment]:
        """
        è¯­ä¹‰è§£æ gs.mdï¼Œæå–é€å­—ç¨¿æ®µè½ã€‚
        
        ä½¿ç”¨ LLM ç†è§£æ–‡æ¡£ç»“æ„ï¼š
        1. è¯†åˆ«"å®Œæ•´é€å­—ç¨¿"éƒ¨åˆ†ï¼ˆä¸»è¦å†…å®¹ï¼‰
        2. æ’é™¤"å›¾åƒè¡¥å……è¯´æ˜"ã€"é‡è¦æœ¯è¯­"ã€"å­¦ä¹ æ”¶è·"ç­‰éé€å­—ç¨¿éƒ¨åˆ†
        3. æå–æ—¶é—´é”šç‚¹å’Œå¯¹åº”çš„è¯´è¯äºº
        4. æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤ Markdown æ ¼å¼ã€æ‹¬å·æ³¨é‡Šç­‰ï¼‰
        """
        
    def extract_speakers(self, gs_path: Path) -> List[str]:
        """æå–æ‰€æœ‰å”¯ä¸€è¯´è¯äººåç§°ã€‚"""
        
    def identify_sections(self, content: str) -> Dict[str, str]:
        """
        è¯­ä¹‰è¯†åˆ«æ–‡æ¡£å„éƒ¨åˆ†ï¼š
        - åŸºæœ¬ä¿¡æ¯
        - å®Œæ•´é€å­—ç¨¿ï¼ˆä¸»è¦å†…å®¹ï¼‰
        - å›¾åƒè¡¥å……è¯´æ˜ï¼ˆæ’é™¤ï¼‰
        - é‡è¦æœ¯è¯­å’Œäººç‰©ï¼ˆæ’é™¤ï¼‰
        - å­¦ä¹ æ”¶è·ï¼ˆæ’é™¤ï¼‰
        """
```

**è¯­ä¹‰è§£æç­–ç•¥ï¼š**

1. **æ–‡æ¡£ç»“æ„è¯†åˆ«**ï¼š
   - è¯†åˆ« `## å®Œæ•´é€å­—ç¨¿` éƒ¨åˆ†ä½œä¸ºä¸»è¦å†…å®¹æ¥æº
   - æ’é™¤ `## ğŸ” å›¾åƒè¡¥å……è¯´æ˜`ã€`## ğŸ“š é‡è¦æœ¯è¯­`ã€`## ğŸ’¡ å­¦ä¹ æ”¶è·` ç­‰è¾…åŠ©éƒ¨åˆ†
   - å¤„ç† `## å®Œæ•´é€å­—ç¨¿ï¼ˆç»§ç»­ Q&A éƒ¨åˆ†ï¼‰` ç­‰å»¶ç»­éƒ¨åˆ†

2. **æ—¶é—´é”šç‚¹æå–**ï¼š
   - æ ¼å¼ï¼š`### [MM:SS] Speaker_Name`
   - è¯´è¯äººåç§°æ”¯æŒä¸­è‹±æ–‡æ··åˆï¼ˆå¦‚ `è§‚ä¼—æé—® 2 (Janet)`ï¼‰

3. **å†…å®¹æ¸…ç†**ï¼š
   - ç§»é™¤æ‹¬å·ä¸­çš„è‹±æ–‡åŸæ–‡ï¼ˆå¦‚ `ä¿®è¾å­¦ï¼ˆRhetoricï¼‰` â†’ `ä¿®è¾å­¦`ï¼‰
   - ä¿ç•™å¿…è¦çš„æœ¯è¯­ç¿»è¯‘å¯¹ç…§
   - ç§»é™¤å›¾åƒæè¿°è¡Œï¼ˆ`**[MM:SS]** ç”»é¢å†…å®¹ï¼š...`ï¼‰

4. **LLM è¾…åŠ©ç†è§£**ï¼š
   - å½“æ–‡æ¡£ç»“æ„å¤æ‚æ—¶ï¼Œä½¿ç”¨ LLM ç†è§£æ®µè½è¾¹ç•Œ
   - è¯†åˆ«è¯´è¯äººåˆ‡æ¢çš„è¯­ä¹‰çº¿ç´¢
   - å¤„ç†éæ ‡å‡†æ ¼å¼çš„æ—¶é—´æ ‡è®°

### 2. Speaker Tracker (`SpeakerTracker`)

Tracks current speaker and manages speaker-to-voice mappings.

```python
class SpeakerTracker:
    def __init__(self, voice_map_path: Optional[Path] = None):
        self.voice_map: Dict[str, str] = {}
        self.current_speaker: str = "DEFAULT"
        
    def update_speaker(self, timestamp_ms: int, gs_segments: List[GSSegment]) -> str:
        """Get speaker for given timestamp based on gs.md anchors."""
        
    def get_voice(self, speaker: str) -> str:
        """Get TTS voice for speaker from voice_map."""
        
    def generate_voice_map(self, speakers: List[str]) -> Dict[str, str]:
        """Generate voice_map.json template with all speakers."""
```

### 3. Alignment Engine (`AlignmentEngine`)

Core component that maps gs.md translations to original SRT timeline.

```python
class AlignmentEngine:
    def __init__(
        self,
        gs_segments: List[GSSegment],
        original_subs: List[srt.Subtitle],
        glossary: Optional[Dict[str, str]] = None
    ):
        pass
        
    def align(self) -> List[AlignedSubtitle]:
        """
        Align gs.md content to original SRT timeline.
        
        Algorithm:
        1. For each original SRT entry, find the gs.md segment that covers it
        2. If covered, use gs.md translation
        3. If not covered (beyond last anchor), use original SRT text as fallback
        4. Track speaker changes based on gs.md anchors
        """
        
    def get_coverage_stats(self) -> CoverageStats:
        """Return statistics about gs.md coverage."""
```

**Alignment Algorithm:**

```
For each original_sub in original_subs:
    sub_start_ms = original_sub.start.total_seconds() * 1000
    
    # Find which gs segment covers this timestamp
    covering_segment = None
    for i, gs_seg in enumerate(gs_segments):
        gs_start = gs_seg.start_ms
        gs_end = gs_segments[i+1].start_ms if i+1 < len(gs_segments) else infinity
        
        if gs_start <= sub_start_ms < gs_end:
            covering_segment = gs_seg
            break
    
    if covering_segment:
        # Use gs.md translation, distribute text proportionally
        text = distribute_text(covering_segment, original_sub, all_subs_in_segment)
        speaker = covering_segment.speaker
    else:
        # Fallback to original SRT (beyond gs.md coverage)
        text = original_sub.content
        speaker = last_known_speaker
        mark_as_fallback()
    
    output.append(AlignedSubtitle(
        index=original_sub.index,
        start=original_sub.start,
        end=original_sub.end,
        text=text,
        speaker=speaker
    ))
```

### 4. Text Splitter (`TextSplitter`)

Handles text processing for TTS optimization.

```python
class TextSplitter:
    MAX_CHARS = 75  # Doubao TTS limit
    
    def clean_markdown(self, text: str) -> str:
        """Remove markdown formatting, image descriptions, etc."""
        
    def split_for_tts(self, text: str, max_chars: int = MAX_CHARS) -> List[str]:
        """Split text at natural boundaries (ã€‚ï¼ï¼Ÿï¼Œï¼›) if exceeding limit."""
        
    def remove_fillers(self, text: str) -> str:
        """Remove oral fillers like å—¯ã€å•Šã€å‘ƒ."""
```

### 5. Output Generator

Generates the final audio.srt with optional speaker tags.

```python
def generate_audio_srt(
    aligned_subs: List[AlignedSubtitle],
    include_speaker_tags: bool = True
) -> str:
    """
    Generate SRT content with optional speaker tags.
    
    Format with tags: "[Speaker: Ian Bogost] è¿™æ˜¯ç¿»è¯‘æ–‡æœ¬"
    Format without tags: "è¿™æ˜¯ç¿»è¯‘æ–‡æœ¬"
    """
```

## Data Models

```python
@dataclass
class GSSegment:
    start_ms: int
    speaker: str
    text: str

@dataclass
class AlignedSubtitle:
    index: int
    start: timedelta
    end: timedelta
    text: str
    speaker: str
    is_fallback: bool = False

@dataclass
class CoverageStats:
    total_entries: int
    covered_entries: int
    fallback_entries: int
    coverage_percent: float
    last_anchor_time: str
    video_duration: str
```

## æ­£ç¡®æ€§å±æ€§

*æ­£ç¡®æ€§å±æ€§æ˜¯æŒ‡åœ¨ç³»ç»Ÿæ‰€æœ‰æœ‰æ•ˆæ‰§è¡Œä¸­éƒ½åº”ä¿æŒä¸ºçœŸçš„ç‰¹å¾æˆ–è¡Œä¸ºâ€”â€”æœ¬è´¨ä¸Šæ˜¯å…³äºç³»ç»Ÿåº”è¯¥åšä»€ä¹ˆçš„å½¢å¼åŒ–é™ˆè¿°ã€‚å±æ€§æ˜¯äººç±»å¯è¯»è§„èŒƒä¸æœºå™¨å¯éªŒè¯æ­£ç¡®æ€§ä¿è¯ä¹‹é—´çš„æ¡¥æ¢ã€‚*

### Property 1: æ¡ç›®æ•°é‡ä¿æŒä¸å˜

*å¯¹äºä»»æ„* æœ‰æ•ˆçš„ gs.md å’ŒåŸå§‹ SRT è¾“å…¥å¯¹ï¼Œç”Ÿæˆçš„ audio.srt æ¡ç›®æ•°é‡åº”ä¸åŸå§‹ SRT æ¡ç›®æ•°é‡å®Œå…¨ç›¸ç­‰ã€‚

**Validates: Requirements 1.1, 1.3**

### Property 2: æœªè¦†ç›–æ®µè½çš„å›é€€è¡Œä¸º

*å¯¹äºä»»æ„* gs.md æœ€åä¸€ä¸ªæ—¶é—´é”šç‚¹ä¹‹åçš„ SRT æ¡ç›®ï¼Œå…¶æ–‡æœ¬å†…å®¹åº”æ¥è‡ªåŸå§‹ SRT çš„å¯¹åº”æ¡ç›®ã€‚

**Validates: Requirements 1.2, 6.1**

### Property 3: æ®µè½åˆ°é”šç‚¹çš„æ­£ç¡®æ˜ å°„

*å¯¹äºä»»æ„* ä½äºä¸¤ä¸ª gs.md æ—¶é—´é”šç‚¹ä¹‹é—´çš„ SRT æ¡ç›®ï¼Œå…¶ç¿»è¯‘æ–‡æœ¬åº”æ¥è‡ªå‰ä¸€ä¸ªé”šç‚¹å¯¹åº”çš„ gs.md å†…å®¹æ®µè½ã€‚

**Validates: Requirements 1.4**

### Property 4: è¯´è¯äººåç§°æå–

*å¯¹äºä»»æ„* æ ¼å¼ä¸º `### [MM:SS] Speaker_Name` çš„ gs.md é”šç‚¹ï¼Œè§£æå™¨åº”æ­£ç¡®æå–è¯´è¯äººåç§°ï¼ˆé”šç‚¹ä¸­æ—¶é—´æˆ³ä¹‹åçš„æ‰€æœ‰å†…å®¹ï¼‰ã€‚

**Validates: Requirements 2.1**

### Property 5: è¯´è¯äººæ ‡ç­¾ä¼ æ’­

*å¯¹äºä»»æ„* è¯´è¯äººé”šç‚¹ä¹‹åçš„ SRT æ¡ç›®åºåˆ—ï¼Œåœ¨é‡åˆ°ä¸‹ä¸€ä¸ªè¯´è¯äººé”šç‚¹ä¹‹å‰ï¼Œæ‰€æœ‰æ¡ç›®éƒ½åº”æ ‡è®°ä¸ºå½“å‰è¯´è¯äººï¼Œä¸”è¾“å‡ºæ ¼å¼åº”ä¸º `[Speaker: Name] Text`ã€‚

**Validates: Requirements 2.3, 2.4, 7.4**

### Property 6: é»˜è®¤éŸ³è‰²å›é€€

*å¯¹äºä»»æ„* ä¸åœ¨ voice_map.json ä¸­çš„è¯´è¯äººåç§°ï¼Œç³»ç»Ÿåº”è¿”å› DEFAULT å¯¹åº”çš„éŸ³è‰²ã€‚

**Validates: Requirements 3.2**

### Property 7: è¯´è¯äººéªŒè¯å®Œæ•´æ€§

*å¯¹äºä»»æ„* gs.md ä¸­å‡ºç°çš„è¯´è¯äººï¼Œå¦‚æœå…¶ä¸åœ¨ voice_map.json ä¸­ï¼Œç³»ç»Ÿåº”ç”Ÿæˆè­¦å‘Šä¿¡æ¯ã€‚

**Validates: Requirements 3.3**

### Property 8: éŸ³è‰²æ˜ å°„ç”Ÿæˆå®Œæ•´æ€§

*å¯¹äºä»»æ„* gs.md æ–‡ä»¶ï¼Œç”Ÿæˆçš„ voice_map.json åº”åŒ…å«è¯¥æ–‡ä»¶ä¸­æ‰€æœ‰å”¯ä¸€çš„è¯´è¯äººåç§°ã€‚

**Validates: Requirements 3.4**

### Property 9: æ–‡æœ¬æ¸…ç†ï¼ˆMarkdownã€å›¾ç‰‡æè¿°ã€å£è¯­å¡«å……è¯ï¼‰

*å¯¹äºä»»æ„* åŒ…å« Markdown æ ¼å¼ï¼ˆ**ç²—ä½“**ã€# æ ‡é¢˜ã€- åˆ—è¡¨ï¼‰ã€å›¾ç‰‡æè¿°ï¼ˆ`**[MM:SS]** ç”»é¢å†…å®¹ï¼š...`ï¼‰æˆ–å£è¯­å¡«å……è¯ï¼ˆå—¯ã€å•Šã€å‘ƒï¼‰çš„è¾“å…¥æ–‡æœ¬ï¼Œè¾“å‡ºæ–‡æœ¬åº”ä¸åŒ…å«è¿™äº›å†…å®¹ã€‚

**Validates: Requirements 4.1, 4.2, 4.5**

### Property 10: æ®µè½åˆ†å‰²éµå®ˆå­—ç¬¦é™åˆ¶

*å¯¹äºä»»æ„* è¶…è¿‡ 75 å­—ç¬¦çš„æ–‡æœ¬æ®µè½ï¼Œåˆ†å‰²åçš„æ¯ä¸ªå­æ®µè½é•¿åº¦åº” â‰¤75 å­—ç¬¦ï¼Œä¸”åˆ†å‰²ç‚¹åº”ä½äºè‡ªç„¶æ ‡ç‚¹ç¬¦å·å¤„ï¼ˆã€‚ï¼ï¼Ÿï¼Œï¼›ï¼‰ã€‚

**Validates: Requirements 4.3**

### Property 11: æœ¯è¯­ä¸€è‡´æ€§æ£€æŸ¥

*å¯¹äºä»»æ„* å­˜åœ¨ glossary.yaml çš„é¡¹ç›®ï¼Œç³»ç»Ÿåº”æ£€æµ‹ gs.md ä¸­ä¸æœ¯è¯­è¡¨ä¸ä¸€è‡´çš„ç¿»è¯‘ç”¨æ³•ã€‚

**Validates: Requirements 5.2**

### Property 12: è¦†ç›–ç‡ç»Ÿè®¡å‡†ç¡®æ€§

*å¯¹äºä»»æ„* gs.md å’Œ SRT è¾“å…¥å¯¹ï¼Œè®¡ç®—çš„è¦†ç›–ç‡ç™¾åˆ†æ¯”åº”ç­‰äº (è¢« gs.md è¦†ç›–çš„æ¡ç›®æ•° / æ€»æ¡ç›®æ•°) Ã— 100ã€‚

**Validates: Requirements 6.3**

### Property 13: æœ‰æ•ˆ SRT è¾“å‡ºæ ¼å¼

*å¯¹äºä»»æ„* ç”Ÿæˆçš„ audio.srt è¾“å‡ºï¼Œä½¿ç”¨ `srt.parse()` è§£æåº”æˆåŠŸä¸”ä¸æŠ›å‡ºå¼‚å¸¸ã€‚

**Validates: Requirements 7.1**

### Property 14: åŸå§‹ç»“æ„ä¿æŒä¸å˜

*å¯¹äºä»»æ„* åŸå§‹ SRT è¾“å…¥ï¼Œç”Ÿæˆçš„ audio.srt åº”ä¿æŒå®Œå…¨ç›¸åŒçš„ç´¢å¼•å·å’Œæ—¶é—´æˆ³ã€‚

**Validates: Requirements 7.2, 7.3**

## é”™è¯¯å¤„ç†

| é”™è¯¯åœºæ™¯ | å¤„ç†æ–¹å¼ |
|---------|---------|
| gs.md æ–‡ä»¶ä¸å­˜åœ¨ | æŠ›å‡º FileNotFoundErrorï¼Œæç¤ºç”¨æˆ·åˆ›å»º gs.md |
| gs.md æ— æœ‰æ•ˆæ—¶é—´é”šç‚¹ | æŠ›å‡º ValueErrorï¼Œæç¤ºé”šç‚¹æ ¼å¼è¦æ±‚ |
| åŸå§‹ SRT è§£æå¤±è´¥ | æŠ›å‡º srt.SRTParseErrorï¼Œæ˜¾ç¤ºå…·ä½“è¡Œå· |
| voice_map.json æ ¼å¼é”™è¯¯ | æŠ›å‡º JSONDecodeErrorï¼Œä½¿ç”¨é»˜è®¤æ˜ å°„å¹¶è­¦å‘Š |
| glossary.yaml æ ¼å¼é”™è¯¯ | è­¦å‘Šå¹¶è·³è¿‡æœ¯è¯­æ£€æŸ¥ |
| gs.md è¦†ç›–ç‡ < 80% | å‘å‡ºè­¦å‘Šä½†ç»§ç»­å¤„ç† |
| å•ä¸ªæ®µè½è¶…è¿‡ 250 å­—ç¬¦ | å¼ºåˆ¶åˆ†å‰²å¹¶è®°å½•è­¦å‘Š |

## æµ‹è¯•ç­–ç•¥

### å•å…ƒæµ‹è¯•

- **GSParser æµ‹è¯•**: éªŒè¯å„ç§é”šç‚¹æ ¼å¼çš„è§£æ
- **SpeakerTracker æµ‹è¯•**: éªŒè¯è¯´è¯äººåˆ‡æ¢å’ŒéŸ³è‰²æŸ¥æ‰¾
- **TextSplitter æµ‹è¯•**: éªŒè¯ Markdown æ¸…ç†å’Œé•¿æ–‡æœ¬åˆ†å‰²
- **AlignmentEngine æµ‹è¯•**: éªŒè¯å¯¹é½ç®—æ³•çš„è¾¹ç•Œæƒ…å†µ

### å±æ€§æµ‹è¯•

ä½¿ç”¨ `hypothesis` åº“è¿›è¡Œå±æ€§æµ‹è¯•ï¼Œæ¯ä¸ªå±æ€§è‡³å°‘è¿è¡Œ 100 æ¬¡è¿­ä»£ã€‚

```python
# æµ‹è¯•æ¡†æ¶é…ç½®
import hypothesis
from hypothesis import given, strategies as st, settings

@settings(max_examples=100)
```

**æµ‹è¯•æ ‡ç­¾æ ¼å¼**: `Feature: gs-align-multi-speaker, Property {number}: {property_text}`

### é›†æˆæµ‹è¯•

- ä½¿ç”¨ Ian Bogost è§†é¢‘é¡¹ç›®ä½œä¸ºç«¯åˆ°ç«¯æµ‹è¯•ç”¨ä¾‹
- éªŒè¯ 414 æ¡ç›®å®Œæ•´ç”Ÿæˆ
- éªŒè¯ 3 ä¸ªè¯´è¯äººæ­£ç¡®è¯†åˆ«
- éªŒè¯è¦†ç›–ç‡æŠ¥å‘Šå‡†ç¡®æ€§
