"""
GS è¯­ä¹‰çŸ«æ­£ SRT ç¿»è¯‘æ¨¡å—

ä½¿ç”¨ gs.md ä½œä¸ºè¯­ä¹‰èƒŒæ™¯ä¸Šä¸‹æ–‡ï¼Œé€šè¿‡ LLM é€æ­¥çŸ«æ­£ SRT ç¿»è¯‘è´¨é‡ã€‚
æ ¸å¿ƒè®¾è®¡ç†å¿µï¼š
- gs.md æ˜¯èƒŒæ™¯å‚è€ƒä¿¡æ¯ï¼Œä¸æ˜¯ç›´æ¥æ›¿æ¢ SRT çš„æ¥æº
- ä½¿ç”¨è¯­ä¹‰ç†è§£è€Œéæœºæ¢°å¯¹é½æ¥çŸ«æ­£ç¿»è¯‘
- åˆ†æ®µå¤„ç†å¤§æ–‡ä»¶ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
- æœ¬åœ°åŒ–å®¡æŸ¥ç¡®ä¿ä¸­å›½äººå¯ç†è§£
"""

from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any
from enum import Enum
import json
import os

from flexdub.core.subtitle import SRTItem


# =============================================================================
# æ ¸å¿ƒæ•°æ®æ¨¡å‹
# =============================================================================

@dataclass
class SpeakerProfile:
    """è¯´è¯äººæ¡£æ¡ˆ"""
    name: str
    role: str = ""                      # å¦‚ï¼šä¸»è®²äººã€è§‚ä¼—æé—®
    speaking_style: str = ""            # è¯´è¯é£æ ¼æè¿°
    first_appearance_ms: int = 0        # é¦–æ¬¡å‡ºç°æ—¶é—´


@dataclass
class SemanticContext:
    """gs.md æå–çš„è¯­ä¹‰ä¸Šä¸‹æ–‡"""
    core_topic: str = ""                # æ ¸å¿ƒä¸»é¢˜
    domain: str = ""                    # é¢†åŸŸï¼ˆå¦‚ï¼šæ¸¸æˆè®¾è®¡ã€ä¿®è¾å­¦ï¼‰
    terminology: Dict[str, str] = field(default_factory=dict)  # æœ¯è¯­æ˜ å°„ {è‹±æ–‡: ä¸­æ–‡}
    speakers: List[SpeakerProfile] = field(default_factory=list)  # è¯´è¯äººä¿¡æ¯
    key_concepts: List[str] = field(default_factory=list)  # å…³é”®æ¦‚å¿µåˆ—è¡¨
    translation_style: str = ""         # ç¿»è¯‘é£æ ¼æè¿°
    raw_content: str = ""               # åŸå§‹å†…å®¹ï¼ˆä¾› LLM å‚è€ƒï¼‰


@dataclass
class RefinedSRTItem:
    """çŸ«æ­£åçš„ SRT æ¡ç›®ï¼ˆæ‰©å±• SRTItemï¼‰"""
    start_ms: int
    end_ms: int
    text: str
    speaker: Optional[str] = None
    is_refined: bool = False
    original_text: str = ""             # åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºå¯¹æ¯”ï¼‰
    
    @classmethod
    def from_srt_item(cls, item: SRTItem) -> "RefinedSRTItem":
        """ä» SRTItem åˆ›å»º"""
        return cls(
            start_ms=item.start_ms,
            end_ms=item.end_ms,
            text=item.text,
            original_text=item.text
        )
    
    def to_srt_item(self) -> SRTItem:
        """è½¬æ¢ä¸º SRTItem"""
        return SRTItem(
            start_ms=self.start_ms,
            end_ms=self.end_ms,
            text=self.text
        )


@dataclass
class Chunk:
    """å¤„ç†å•å…ƒ"""
    index: int
    items: List[RefinedSRTItem]
    start_ms: int
    end_ms: int
    context_summary: str = ""           # å‰ä¸€ä¸ª chunk çš„ä¸Šä¸‹æ–‡æ‘˜è¦
    terminology_used: Dict[str, str] = field(default_factory=dict)  # æœ¬ chunk ä½¿ç”¨çš„æœ¯è¯­
    
    @property
    def item_count(self) -> int:
        """æ¡ç›®æ•°é‡"""
        return len(self.items)
    
    @property
    def duration_ms(self) -> int:
        """æ—¶é•¿ï¼ˆæ¯«ç§’ï¼‰"""
        return self.end_ms - self.start_ms


class IssueSeverity(Enum):
    """é—®é¢˜ä¸¥é‡ç¨‹åº¦"""
    WARNING = "warning"
    ERROR = "error"


@dataclass
class LocalizationIssue:
    """æœ¬åœ°åŒ–é—®é¢˜"""
    index: int                          # SRT æ¡ç›®ç´¢å¼•
    issue_type: str                     # é—®é¢˜ç±»å‹
    original: str                       # åŸæ–‡
    suggestion: str                     # å»ºè®®
    severity: IssueSeverity = IssueSeverity.WARNING


@dataclass
class ProcessingState:
    """å¤„ç†çŠ¶æ€ï¼ˆç”¨äºä¸­æ–­æ¢å¤ï¼‰"""
    total_chunks: int
    completed_chunks: int
    current_chunk_index: int
    terminology: Dict[str, str] = field(default_factory=dict)
    last_context_summary: str = ""
    checkpoint_path: str = ""
    
    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸ï¼ˆç”¨äº JSON åºåˆ—åŒ–ï¼‰"""
        return {
            "total_chunks": self.total_chunks,
            "completed_chunks": self.completed_chunks,
            "current_chunk_index": self.current_chunk_index,
            "terminology": self.terminology,
            "last_context_summary": self.last_context_summary,
            "checkpoint_path": self.checkpoint_path
        }
    
    @classmethod
    def from_dict(cls, data: dict) -> "ProcessingState":
        """ä»å­—å…¸åˆ›å»º"""
        return cls(
            total_chunks=data.get("total_chunks", 0),
            completed_chunks=data.get("completed_chunks", 0),
            current_chunk_index=data.get("current_chunk_index", 0),
            terminology=data.get("terminology", {}),
            last_context_summary=data.get("last_context_summary", ""),
            checkpoint_path=data.get("checkpoint_path", "")
        )
    
    def save(self, path: str) -> None:
        """ä¿å­˜åˆ°æ–‡ä»¶"""
        with open(path, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, ensure_ascii=False, indent=2)
    
    @classmethod
    def load(cls, path: str) -> Optional["ProcessingState"]:
        """ä»æ–‡ä»¶åŠ è½½"""
        if not os.path.exists(path):
            return None
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
            return cls.from_dict(data)
        except (json.JSONDecodeError, KeyError):
            return None


@dataclass
class RefineResult:
    """çŸ«æ­£ç»“æœ"""
    items: List[RefinedSRTItem]
    terminology_used: Dict[str, str] = field(default_factory=dict)
    issues: List[LocalizationIssue] = field(default_factory=list)
    processing_log: str = ""
    
    @property
    def item_count(self) -> int:
        """æ¡ç›®æ•°é‡"""
        return len(self.items)
    
    @property
    def refined_count(self) -> int:
        """å·²çŸ«æ­£çš„æ¡ç›®æ•°é‡"""
        return sum(1 for item in self.items if item.is_refined)
    
    @property
    def issue_count(self) -> int:
        """é—®é¢˜æ•°é‡"""
        return len(self.issues)
    
    def to_srt_items(self) -> List[SRTItem]:
        """è½¬æ¢ä¸º SRTItem åˆ—è¡¨"""
        return [item.to_srt_item() for item in self.items]


# =============================================================================
# è¾…åŠ©å‡½æ•°
# =============================================================================

def srt_items_to_refined(items: List[SRTItem]) -> List[RefinedSRTItem]:
    """å°† SRTItem åˆ—è¡¨è½¬æ¢ä¸º RefinedSRTItem åˆ—è¡¨"""
    return [RefinedSRTItem.from_srt_item(item) for item in items]


def refined_to_srt_items(items: List[RefinedSRTItem]) -> List[SRTItem]:
    """å°† RefinedSRTItem åˆ—è¡¨è½¬æ¢ä¸º SRTItem åˆ—è¡¨"""
    return [item.to_srt_item() for item in items]


# =============================================================================
# Context Extractorï¼ˆä¸Šä¸‹æ–‡æå–å™¨ï¼‰
# =============================================================================

import re


def _parse_timestamp_to_ms(timestamp: str) -> int:
    """
    è§£ææ—¶é—´æˆ³å­—ç¬¦ä¸²ä¸ºæ¯«ç§’
    
    æ”¯æŒæ ¼å¼ï¼š
    - MM:SS
    - HH:MM:SS
    - MM:SS.mmm
    """
    timestamp = timestamp.strip()
    
    # å¤„ç†æ¯«ç§’éƒ¨åˆ†
    ms = 0
    if '.' in timestamp:
        timestamp, ms_str = timestamp.rsplit('.', 1)
        ms = int(ms_str.ljust(3, '0')[:3])
    
    parts = timestamp.split(':')
    if len(parts) == 2:
        minutes, seconds = int(parts[0]), int(parts[1])
        return (minutes * 60 + seconds) * 1000 + ms
    elif len(parts) == 3:
        hours, minutes, seconds = int(parts[0]), int(parts[1]), int(parts[2])
        return (hours * 3600 + minutes * 60 + seconds) * 1000 + ms
    return 0


class ContextExtractor:
    """
    ä» gs.md æå–è¯­ä¹‰ä¸Šä¸‹æ–‡
    
    æ”¯æŒä¸å›ºå®šçš„ gs.md ç»“æ„ï¼Œé€šè¿‡æ¨¡å¼è¯†åˆ«æå–ï¼š
    - æ ¸å¿ƒä¸»é¢˜å’Œé¢†åŸŸ
    - æœ¯è¯­ç¿»è¯‘å¯¹ç…§
    - è¯´è¯äººåŠå…¶é£æ ¼
    - å…³é”®æ¦‚å¿µ
    """
    
    # æ—¶é—´æˆ³+è¯´è¯äººæ ¼å¼ï¼š### [MM:SS] Speaker_Name æˆ– ### [HH:MM:SS] Speaker_Name
    SPEAKER_PATTERN = re.compile(
        r'^###\s*\[(\d{1,2}:\d{2}(?::\d{2})?)\]\s*(.+?)\s*$',
        re.MULTILINE
    )
    
    # æœ¯è¯­æ ¼å¼ï¼šè‹±æ–‡ï¼ˆä¸­æ–‡ï¼‰æˆ– è‹±æ–‡ (ä¸­æ–‡)
    TERMINOLOGY_PATTERN = re.compile(
        r'\b([A-Za-z][A-Za-z\s\-\']+?)\s*[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)]'
    )
    
    # ç²—ä½“æœ¯è¯­æ ¼å¼ï¼š**æœ¯è¯­**
    BOLD_TERM_PATTERN = re.compile(r'\*\*([^*]+)\*\*')
    
    def __init__(self, llm_client: Optional[Any] = None):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡æå–å™¨
        
        Args:
            llm_client: å¯é€‰çš„ LLM å®¢æˆ·ç«¯ï¼Œç”¨äºæ›´æ™ºèƒ½çš„æå–
        """
        self.llm = llm_client
    
    def extract(self, gs_content: str) -> SemanticContext:
        """
        ä» gs.md æå–è¯­ä¹‰ä¸Šä¸‹æ–‡
        
        Args:
            gs_content: gs.md æ–‡ä»¶å†…å®¹
            
        Returns:
            SemanticContext å¯¹è±¡
        """
        # æå–å„éƒ¨åˆ†
        core_topic = self._extract_core_topic(gs_content)
        domain = self._extract_domain(gs_content)
        terminology = self.extract_terminology(gs_content)
        speakers = self.extract_speakers(gs_content)
        key_concepts = self._extract_key_concepts(gs_content)
        translation_style = self._extract_translation_style(gs_content)
        
        return SemanticContext(
            core_topic=core_topic,
            domain=domain,
            terminology=terminology,
            speakers=speakers,
            key_concepts=key_concepts,
            translation_style=translation_style,
            raw_content=gs_content
        )
    
    def extract_terminology(self, content: str) -> Dict[str, str]:
        """
        æå–æœ¯è¯­æ˜ å°„
        
        è¯†åˆ«æ¨¡å¼ï¼š
        - è‹±æ–‡ï¼ˆä¸­æ–‡ï¼‰æ ¼å¼
        - æœ¯è¯­è¡¨éƒ¨åˆ†
        - é¦–æ¬¡å‡ºç°çš„ä¸“ä¸šæœ¯è¯­
        
        Args:
            content: gs.md å†…å®¹
            
        Returns:
            æœ¯è¯­æ˜ å°„å­—å…¸ {è‹±æ–‡: ä¸­æ–‡}
        """
        terminology: Dict[str, str] = {}
        
        # 1. æŸ¥æ‰¾ä¸“é—¨çš„æœ¯è¯­è¡¨éƒ¨åˆ†
        terminology.update(self._extract_terminology_section(content))
        
        # 2. æå– è‹±æ–‡ï¼ˆä¸­æ–‡ï¼‰æ ¼å¼çš„æœ¯è¯­
        for match in self.TERMINOLOGY_PATTERN.finditer(content):
            eng = match.group(1).strip()
            chn = match.group(2).strip()
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„åŒ¹é…
            if 2 <= len(eng) <= 50 and 1 <= len(chn) <= 30:
                # é¿å…è¦†ç›–å·²æœ‰çš„æœ¯è¯­
                if eng not in terminology:
                    terminology[eng] = chn
        
        return terminology
    
    def _extract_terminology_section(self, content: str) -> Dict[str, str]:
        """ä»æœ¯è¯­è¡¨éƒ¨åˆ†æå–æœ¯è¯­"""
        terminology: Dict[str, str] = {}
        
        # æŸ¥æ‰¾æœ¯è¯­è¡¨éƒ¨åˆ†ï¼ˆå¸¸è§æ ‡é¢˜ï¼‰
        section_patterns = [
            r'##\s*(?:ğŸ“š\s*)?(?:é‡è¦)?æœ¯è¯­(?:å’Œäººç‰©|è¡¨)?.*?\n(.*?)(?=\n##|\n#\s|\Z)',
            r'##\s*(?:ğŸ“š\s*)?(?:Important\s+)?Terms?.*?\n(.*?)(?=\n##|\n#\s|\Z)',
            r'##\s*Glossary.*?\n(.*?)(?=\n##|\n#\s|\Z)',
        ]
        
        for pattern in section_patterns:
            match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
            if match:
                section = match.group(1)
                # è§£æåˆ—è¡¨é¡¹ï¼š- **æœ¯è¯­ (ç¿»è¯‘)**ï¼šè§£é‡Š
                list_pattern = re.compile(
                    r'-\s*\*\*([^*]+?)(?:\s*[ï¼ˆ(]([^ï¼‰)]+)[ï¼‰)])?\*\*'
                )
                for item in list_pattern.finditer(section):
                    term = item.group(1).strip()
                    translation = item.group(2)
                    if translation:
                        terminology[term] = translation.strip()
                break
        
        return terminology
    
    def extract_speakers(self, content: str) -> List[SpeakerProfile]:
        """
        æå–è¯´è¯äººä¿¡æ¯
        
        è¯†åˆ«æ¨¡å¼ï¼š
        - ### [MM:SS] Speaker_Name æ ¼å¼
        - åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†çš„è®²è€…æ•°é‡
        
        Args:
            content: gs.md å†…å®¹
            
        Returns:
            è¯´è¯äººæ¡£æ¡ˆåˆ—è¡¨
        """
        speakers: Dict[str, SpeakerProfile] = {}
        
        # ä»æ—¶é—´æˆ³æ ‡é¢˜æå–è¯´è¯äºº
        for match in self.SPEAKER_PATTERN.finditer(content):
            timestamp = match.group(1)
            speaker_name = match.group(2).strip()
            
            # æ¸…ç†è¯´è¯äººåç§°ï¼ˆç§»é™¤å¯èƒ½çš„æ ‡è®°ï¼‰
            speaker_name = re.sub(r'\s*\(.*?\)\s*$', '', speaker_name)
            
            if speaker_name and speaker_name not in speakers:
                ms = _parse_timestamp_to_ms(timestamp)
                # æ¨æ–­è§’è‰²
                role = self._infer_speaker_role(speaker_name, content)
                speakers[speaker_name] = SpeakerProfile(
                    name=speaker_name,
                    role=role,
                    speaking_style="",
                    first_appearance_ms=ms
                )
        
        # æŒ‰é¦–æ¬¡å‡ºç°æ—¶é—´æ’åº
        return sorted(speakers.values(), key=lambda s: s.first_appearance_ms)
    
    def _infer_speaker_role(self, speaker_name: str, content: str) -> str:
        """æ¨æ–­è¯´è¯äººè§’è‰²"""
        name_lower = speaker_name.lower()
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸»è®²äºº
        if 'ä¸»è®²äºº' in content and speaker_name in content.split('ä¸»è®²äºº')[1][:100]:
            return "ä¸»è®²äºº"
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯è§‚ä¼—æé—®
        if 'è§‚ä¼—' in name_lower or 'audience' in name_lower:
            return "è§‚ä¼—æé—®"
        if 'æé—®' in name_lower or 'question' in name_lower:
            return "è§‚ä¼—æé—®"
        
        # é»˜è®¤è§’è‰²
        return ""
    
    def _extract_core_topic(self, content: str) -> str:
        """æå–æ ¸å¿ƒä¸»é¢˜"""
        # ä»æ ‡é¢˜æå–
        title_match = re.search(r'^#\s+(.+?)(?:\s*[-â€“â€”]\s*.+)?$', content, re.MULTILINE)
        if title_match:
            return title_match.group(1).strip()
        
        # ä»åŸºæœ¬ä¿¡æ¯éƒ¨åˆ†æå–
        topic_match = re.search(r'ä¸»é¢˜[ï¼š:]\s*(.+?)(?:\n|$)', content)
        if topic_match:
            return topic_match.group(1).strip()
        
        return ""
    
    def _extract_domain(self, content: str) -> str:
        """æå–é¢†åŸŸ"""
        # ä»å†…å®¹æ¨æ–­é¢†åŸŸ
        domain_keywords = {
            "æ¸¸æˆè®¾è®¡": ["æ¸¸æˆ", "game", "ç©å®¶", "player"],
            "ä¿®è¾å­¦": ["ä¿®è¾", "rhetoric", "è¯´æœ", "persuasion"],
            "åª’ä½“ç†è®º": ["åª’ä½“", "media", "ä¼ æ’­", "communication"],
            "æ•™è‚²": ["æ•™è‚²", "education", "å­¦ä¹ ", "learning"],
            "è®¾è®¡": ["è®¾è®¡", "design", "UX", "ç”¨æˆ·ä½“éªŒ"],
            "æ•°æ®å¯è§†åŒ–": ["å¯è§†åŒ–", "visualization", "æ•°æ®", "data"],
        }
        
        content_lower = content.lower()
        for domain, keywords in domain_keywords.items():
            count = sum(1 for kw in keywords if kw.lower() in content_lower)
            if count >= 2:
                return domain
        
        return ""
    
    def _extract_key_concepts(self, content: str) -> List[str]:
        """æå–å…³é”®æ¦‚å¿µ"""
        concepts: List[str] = []
        
        # ä»ç²—ä½“æ–‡æœ¬æå–
        for match in self.BOLD_TERM_PATTERN.finditer(content):
            term = match.group(1).strip()
            # è¿‡æ»¤æ‰å¤ªçŸ­æˆ–å¤ªé•¿çš„
            if 2 <= len(term) <= 50 and term not in concepts:
                concepts.append(term)
        
        # é™åˆ¶æ•°é‡
        return concepts[:20]
    
    def _extract_translation_style(self, content: str) -> str:
        """æå–ç¿»è¯‘é£æ ¼æè¿°"""
        # æŸ¥æ‰¾ç¿»è¯‘é£æ ¼ç›¸å…³çš„æè¿°
        style_patterns = [
            r'ç¿»è¯‘é£æ ¼[ï¼š:]\s*(.+?)(?:\n|$)',
            r'é£æ ¼[ï¼š:]\s*(.+?)(?:\n|$)',
        ]
        
        for pattern in style_patterns:
            match = re.search(pattern, content)
            if match:
                return match.group(1).strip()
        
        # é»˜è®¤é£æ ¼
        return "å‡†ç¡®ã€è‡ªç„¶ã€æ˜“äºç†è§£"


# =============================================================================
# Chunk Managerï¼ˆåˆ†æ®µç®¡ç†å™¨ï¼‰
# =============================================================================


class ChunkManager:
    """
    ç®¡ç†å¤§æ–‡ä»¶çš„åˆ†æ®µå¤„ç†ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯
    
    ç­–ç•¥ï¼š
    1. é»˜è®¤ 30 æ¡ä¸ºä¸€ä¸ª chunk
    2. åœ¨è‡ªç„¶æ–­ç‚¹ï¼ˆé•¿åœé¡¿ã€è¯´è¯äººåˆ‡æ¢ï¼‰å¤„åˆ†å‰²
    3. é¿å…åœ¨å¥å­ä¸­é—´åˆ†å‰²
    """
    
    DEFAULT_CHUNK_SIZE = 30
    MIN_CHUNK_SIZE = 20
    MAX_CHUNK_SIZE = 50
    
    # å¥å­ç»“æŸæ ‡ç‚¹
    SENTENCE_TERMINATORS = {'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?'}
    
    # é•¿åœé¡¿é˜ˆå€¼ï¼ˆæ¯«ç§’ï¼‰
    LONG_PAUSE_MS = 2000
    
    def __init__(
        self,
        items: List[RefinedSRTItem],
        checkpoint_dir: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–åˆ†æ®µç®¡ç†å™¨
        
        Args:
            items: SRT æ¡ç›®åˆ—è¡¨
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        """
        self.items = items
        self.checkpoint_dir = checkpoint_dir
        self.chunks: List[Chunk] = []
        self.processed_results: Dict[int, List[RefinedSRTItem]] = {}
        self._terminology: Dict[str, str] = {}
        self._last_context_summary: str = ""
    
    def create_chunks(self) -> List[Chunk]:
        """
        åˆ›å»ºå¤„ç† chunks
        
        Returns:
            Chunk åˆ—è¡¨
        """
        if not self.items:
            return []
        
        # å¦‚æœæ¡ç›®æ•°å°‘äºæœ€å° chunk å¤§å°ï¼Œç›´æ¥è¿”å›å•ä¸ª chunk
        if len(self.items) <= self.MIN_CHUNK_SIZE:
            chunk = Chunk(
                index=0,
                items=self.items.copy(),
                start_ms=self.items[0].start_ms,
                end_ms=self.items[-1].end_ms
            )
            self.chunks = [chunk]
            return self.chunks
        
        # æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
        split_points = self._find_split_points()
        
        # æ ¹æ®åˆ†å‰²ç‚¹åˆ›å»º chunks
        self.chunks = self._create_chunks_from_split_points(split_points)
        
        return self.chunks
    
    def _find_split_points(self) -> List[int]:
        """
        æ‰¾åˆ°æ‰€æœ‰å¯èƒ½çš„åˆ†å‰²ç‚¹
        
        åˆ†å‰²ç‚¹ä¼˜å…ˆçº§ï¼š
        1. é•¿åœé¡¿ï¼ˆ>2ç§’ï¼‰
        2. å¥å­ç»“æŸ
        3. é»˜è®¤ä½ç½®
        
        Returns:
            åˆ†å‰²ç‚¹ç´¢å¼•åˆ—è¡¨
        """
        split_points: List[Tuple[int, int]] = []  # (index, priority)
        
        for i in range(len(self.items) - 1):
            current = self.items[i]
            next_item = self.items[i + 1]
            
            # è®¡ç®—åœé¡¿æ—¶é•¿
            pause_ms = next_item.start_ms - current.end_ms
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯å¥å­ç»“æŸ
            text = current.text.strip()
            is_sentence_end = text and text[-1] in self.SENTENCE_TERMINATORS
            
            # ç¡®å®šä¼˜å…ˆçº§ï¼ˆæ•°å­—è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜ï¼‰
            if pause_ms >= self.LONG_PAUSE_MS and is_sentence_end:
                priority = 1  # æœ€ä½³åˆ†å‰²ç‚¹
            elif pause_ms >= self.LONG_PAUSE_MS:
                priority = 2
            elif is_sentence_end:
                priority = 3
            else:
                priority = 4
            
            split_points.append((i, priority))
        
        return split_points
    
    def _create_chunks_from_split_points(
        self,
        split_points: List[Tuple[int, int]]
    ) -> List[Chunk]:
        """
        æ ¹æ®åˆ†å‰²ç‚¹åˆ›å»º chunks
        
        Args:
            split_points: (ç´¢å¼•, ä¼˜å…ˆçº§) åˆ—è¡¨
            
        Returns:
            Chunk åˆ—è¡¨
        """
        chunks: List[Chunk] = []
        start_idx = 0
        chunk_index = 0
        
        while start_idx < len(self.items):
            # è®¡ç®—ç†æƒ³çš„ç»“æŸä½ç½®
            ideal_end = start_idx + self.DEFAULT_CHUNK_SIZE - 1
            
            # å¦‚æœå‰©ä½™æ¡ç›®ä¸å¤šï¼Œç›´æ¥åŒ…å«åˆ°å½“å‰ chunk
            if ideal_end >= len(self.items) - self.MIN_CHUNK_SIZE:
                end_idx = len(self.items) - 1
            else:
                # åœ¨ç†æƒ³ä½ç½®é™„è¿‘æ‰¾æœ€ä½³åˆ†å‰²ç‚¹
                end_idx = self._find_best_split_point(
                    split_points, start_idx, ideal_end
                )
            
            # åˆ›å»º chunk
            chunk_items = self.items[start_idx:end_idx + 1]
            chunk = Chunk(
                index=chunk_index,
                items=chunk_items,
                start_ms=chunk_items[0].start_ms,
                end_ms=chunk_items[-1].end_ms
            )
            chunks.append(chunk)
            
            # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ª chunk
            start_idx = end_idx + 1
            chunk_index += 1
        
        return chunks
    
    def _find_best_split_point(
        self,
        split_points: List[Tuple[int, int]],
        start_idx: int,
        ideal_end: int
    ) -> int:
        """
        åœ¨æŒ‡å®šèŒƒå›´å†…æ‰¾åˆ°æœ€ä½³åˆ†å‰²ç‚¹
        
        Args:
            split_points: æ‰€æœ‰åˆ†å‰²ç‚¹
            start_idx: å½“å‰ chunk èµ·å§‹ç´¢å¼•
            ideal_end: ç†æƒ³ç»“æŸç´¢å¼•
            
        Returns:
            æœ€ä½³åˆ†å‰²ç‚¹ç´¢å¼•
        """
        min_end = start_idx + self.MIN_CHUNK_SIZE - 1
        max_end = min(start_idx + self.MAX_CHUNK_SIZE - 1, len(self.items) - 1)
        
        # ç¡®ä¿èŒƒå›´æœ‰æ•ˆ
        min_end = max(min_end, start_idx)
        max_end = min(max_end, len(self.items) - 1)
        
        # åœ¨èŒƒå›´å†…æ‰¾ä¼˜å…ˆçº§æœ€é«˜çš„åˆ†å‰²ç‚¹
        best_idx = ideal_end
        best_priority = 999
        
        for idx, priority in split_points:
            if min_end <= idx <= max_end:
                # ä¼˜å…ˆçº§ç›¸åŒæ—¶ï¼Œé€‰æ‹©æ›´æ¥è¿‘ç†æƒ³ä½ç½®çš„
                if priority < best_priority or (
                    priority == best_priority and 
                    abs(idx - ideal_end) < abs(best_idx - ideal_end)
                ):
                    best_idx = idx
                    best_priority = priority
        
        return min(best_idx, max_end)
    
    def get_context_for_chunk(self, chunk_index: int) -> str:
        """
        è·å–å½“å‰ chunk çš„ä¸Šä¸‹æ–‡
        
        åŒ…å«ï¼š
        - å‰ä¸€ä¸ª chunk çš„æ‘˜è¦
        - å·²ç¡®å®šçš„æœ¯è¯­æ˜ å°„
        - å½“å‰è¯´è¯äºº
        
        Args:
            chunk_index: chunk ç´¢å¼•
            
        Returns:
            ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
        """
        if chunk_index == 0:
            return ""
        
        context_parts = []
        
        # æ·»åŠ å‰æ–‡æ‘˜è¦
        if self._last_context_summary:
            context_parts.append(f"å‰æ–‡æ‘˜è¦ï¼š{self._last_context_summary}")
        
        # æ·»åŠ å·²ä½¿ç”¨çš„æœ¯è¯­
        if self._terminology:
            terms = [f"{eng}={chn}" for eng, chn in list(self._terminology.items())[:10]]
            context_parts.append(f"å·²ç¡®å®šæœ¯è¯­ï¼š{', '.join(terms)}")
        
        return "\n".join(context_parts)
    
    def update_context(
        self,
        chunk_index: int,
        context_summary: str,
        terminology_used: Dict[str, str]
    ) -> None:
        """
        æ›´æ–°ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        Args:
            chunk_index: chunk ç´¢å¼•
            context_summary: æœ¬ chunk çš„ä¸Šä¸‹æ–‡æ‘˜è¦
            terminology_used: æœ¬ chunk ä½¿ç”¨çš„æœ¯è¯­
        """
        self._last_context_summary = context_summary
        self._terminology.update(terminology_used)
        
        # æ›´æ–° chunk çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
        if chunk_index < len(self.chunks):
            self.chunks[chunk_index].context_summary = context_summary
            self.chunks[chunk_index].terminology_used = terminology_used
    
    def save_checkpoint(
        self,
        chunk_index: int,
        result: List[RefinedSRTItem]
    ) -> None:
        """
        ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œæ”¯æŒä¸­æ–­æ¢å¤
        
        Args:
            chunk_index: å·²å®Œæˆçš„ chunk ç´¢å¼•
            result: è¯¥ chunk çš„å¤„ç†ç»“æœ
        """
        self.processed_results[chunk_index] = result
        
        if not self.checkpoint_dir:
            return
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.checkpoint_dir, exist_ok=True)
        
        # ä¿å­˜å¤„ç†çŠ¶æ€
        state = ProcessingState(
            total_chunks=len(self.chunks),
            completed_chunks=chunk_index + 1,
            current_chunk_index=chunk_index + 1,
            terminology=self._terminology,
            last_context_summary=self._last_context_summary,
            checkpoint_path=self.checkpoint_dir
        )
        state.save(os.path.join(self.checkpoint_dir, "state.json"))
        
        # ä¿å­˜å·²å¤„ç†çš„ç»“æœ
        results_data = {}
        for idx, items in self.processed_results.items():
            results_data[str(idx)] = [
                {
                    "start_ms": item.start_ms,
                    "end_ms": item.end_ms,
                    "text": item.text,
                    "speaker": item.speaker,
                    "is_refined": item.is_refined,
                    "original_text": item.original_text
                }
                for item in items
            ]
        
        with open(os.path.join(self.checkpoint_dir, "results.json"), "w", encoding="utf-8") as f:
            json.dump(results_data, f, ensure_ascii=False, indent=2)
    
    def load_checkpoint(self) -> int:
        """
        åŠ è½½æ£€æŸ¥ç‚¹ï¼Œè¿”å›ä¸‹ä¸€ä¸ªå¾…å¤„ç†çš„ chunk ç´¢å¼•
        
        Returns:
            ä¸‹ä¸€ä¸ªå¾…å¤„ç†çš„ chunk ç´¢å¼•ï¼Œå¦‚æœæ²¡æœ‰æ£€æŸ¥ç‚¹åˆ™è¿”å› 0
        """
        if not self.checkpoint_dir:
            return 0
        
        state_path = os.path.join(self.checkpoint_dir, "state.json")
        results_path = os.path.join(self.checkpoint_dir, "results.json")
        
        # åŠ è½½çŠ¶æ€
        state = ProcessingState.load(state_path)
        if not state:
            return 0
        
        self._terminology = state.terminology
        self._last_context_summary = state.last_context_summary
        
        # åŠ è½½å·²å¤„ç†çš„ç»“æœ
        if os.path.exists(results_path):
            try:
                with open(results_path, "r", encoding="utf-8") as f:
                    results_data = json.load(f)
                
                for idx_str, items_data in results_data.items():
                    idx = int(idx_str)
                    items = [
                        RefinedSRTItem(
                            start_ms=item["start_ms"],
                            end_ms=item["end_ms"],
                            text=item["text"],
                            speaker=item.get("speaker"),
                            is_refined=item.get("is_refined", False),
                            original_text=item.get("original_text", "")
                        )
                        for item in items_data
                    ]
                    self.processed_results[idx] = items
            except (json.JSONDecodeError, KeyError):
                pass
        
        return state.current_chunk_index
    
    def get_all_results(self) -> List[RefinedSRTItem]:
        """
        è·å–æ‰€æœ‰å¤„ç†ç»“æœ
        
        Returns:
            æŒ‰é¡ºåºåˆå¹¶çš„æ‰€æœ‰ chunk ç»“æœ
        """
        all_items: List[RefinedSRTItem] = []
        
        for i in range(len(self.chunks)):
            if i in self.processed_results:
                all_items.extend(self.processed_results[i])
            else:
                # æœªå¤„ç†çš„ chunkï¼Œä½¿ç”¨åŸå§‹æ•°æ®
                all_items.extend(self.chunks[i].items)
        
        return all_items
    
    @property
    def terminology(self) -> Dict[str, str]:
        """è·å–ç´¯ç§¯çš„æœ¯è¯­æ˜ å°„"""
        return self._terminology.copy()
    
    @property
    def progress(self) -> float:
        """è·å–å¤„ç†è¿›åº¦ï¼ˆ0-100ï¼‰"""
        if not self.chunks:
            return 0.0
        return len(self.processed_results) / len(self.chunks) * 100


# =============================================================================
# LLM Refinerï¼ˆLLM çŸ«æ­£å™¨ï¼‰
# =============================================================================

import urllib.request
import urllib.error
import time


class LLMRefiner:
    """
    ä½¿ç”¨ LLM è¿›è¡Œç¿»è¯‘çŸ«æ­£çš„æ ¸å¿ƒç»„ä»¶
    
    æ”¯æŒçš„ LLM æä¾›å•†ï¼š
    - OpenAI APIï¼ˆåŒ…æ‹¬å…¼å®¹çš„ APIï¼‰
    - Claude API
    """
    
    MAX_RETRIES = 3
    RETRY_DELAY = 2  # ç§’
    
    # ç³»ç»Ÿæç¤ºæ¨¡æ¿
    SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„ä¸­æ–‡ç¿»è¯‘å®¡æ ¡ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®å‚è€ƒæ–‡æ¡£çŸ«æ­£å­—å¹•ç¿»è¯‘ï¼Œç¡®ä¿ç¿»è¯‘å‡†ç¡®ã€è‡ªç„¶ã€å¯¹ä¸­å›½è§‚ä¼—æ˜“äºç†è§£ã€‚

## çŸ«æ­£è¦æ±‚
1. ä¿æŒåŸå§‹åºå·ä¸å˜
2. ä½¿ç”¨æœ¯è¯­è¡¨ä¸­çš„ç»Ÿä¸€ç¿»è¯‘
3. ç¡®ä¿ç¿»è¯‘å¯¹ä¸­å›½äººè‡ªç„¶å¯ç†è§£
4. é¿å…ç”Ÿç¡¬çš„ç›´è¯‘
5. ä¿æŒè¯´è¯äººçš„è¯­æ°”é£æ ¼
6. æ¯æ¡å­—å¹•ä¸è¶…è¿‡ 75 ä¸ªå­—ç¬¦
7. å¦‚éœ€åˆ†å‰²é•¿å¥ï¼Œåœ¨è‡ªç„¶æ–­ç‚¹å¤„åˆ†å‰²
8. ç§»é™¤æ‰€æœ‰ Markdown æ ¼å¼ï¼ˆ**ç²—ä½“**ã€# æ ‡é¢˜ç­‰ï¼‰

## è¾“å‡ºæ ¼å¼
è¯·ä¸¥æ ¼æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºçŸ«æ­£åçš„å­—å¹•ï¼Œæ¯è¡Œä¸€æ¡ï¼š
[åºå·] çŸ«æ­£åçš„ç¿»è¯‘

ä¾‹å¦‚ï¼š
[1] è¿™æ˜¯ç¬¬ä¸€æ¡çŸ«æ­£åçš„ç¿»è¯‘ã€‚
[2] è¿™æ˜¯ç¬¬äºŒæ¡çŸ«æ­£åçš„ç¿»è¯‘ã€‚"""

    def __init__(
        self,
        semantic_context: SemanticContext,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None
    ):
        """
        åˆå§‹åŒ– LLM çŸ«æ­£å™¨
        
        Args:
            semantic_context: è¯­ä¹‰ä¸Šä¸‹æ–‡
            api_key: API å¯†é’¥ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            base_url: API åŸºç¡€ URLï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
            model: æ¨¡å‹åç§°ï¼ˆé»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
        """
        self.context = semantic_context
        self.api_key = api_key or os.environ.get("FLEXDUB_LLM_API_KEY", "")
        self.base_url = base_url or os.environ.get(
            "FLEXDUB_LLM_BASE_URL", 
            "https://api.openai.com/v1/chat/completions"
        )
        self.model = model or os.environ.get("FLEXDUB_LLM_MODEL", "gpt-4o-mini")
    
    def refine_chunk(
        self,
        chunk: Chunk,
        previous_context: str = ""
    ) -> Tuple[List[RefinedSRTItem], str]:
        """
        çŸ«æ­£ä¸€ä¸ª chunk çš„ç¿»è¯‘
        
        Args:
            chunk: å¾…å¤„ç†çš„ chunk
            previous_context: å‰ä¸€ä¸ª chunk çš„ä¸Šä¸‹æ–‡æ‘˜è¦
            
        Returns:
            (çŸ«æ­£åçš„ SRT æ¡ç›®åˆ—è¡¨, æœ¬ chunk çš„ä¸Šä¸‹æ–‡æ‘˜è¦)
        """
        if not self.api_key:
            # æ²¡æœ‰ API keyï¼Œè¿”å›åŸå§‹æ•°æ®
            return chunk.items, ""
        
        # æ„å»º prompt
        prompt = self.build_prompt(chunk, previous_context)
        
        # è°ƒç”¨ LLM
        response = self._call_llm(prompt)
        
        if not response:
            # LLM è°ƒç”¨å¤±è´¥ï¼Œè¿”å›åŸå§‹æ•°æ®
            return chunk.items, ""
        
        # è§£æå“åº”
        refined_items = self.parse_response(response, chunk.items)
        
        # ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦
        context_summary = self._generate_context_summary(refined_items)
        
        return refined_items, context_summary
    
    def build_prompt(self, chunk: Chunk, previous_context: str = "") -> str:
        """
        æ„å»º LLM prompt
        
        Args:
            chunk: å¾…å¤„ç†çš„ chunk
            previous_context: å‰ä¸€ä¸ª chunk çš„ä¸Šä¸‹æ–‡æ‘˜è¦
            
        Returns:
            å®Œæ•´çš„ prompt å­—ç¬¦ä¸²
        """
        parts = []
        
        # èƒŒæ™¯ä¿¡æ¯
        parts.append("## èƒŒæ™¯ä¿¡æ¯")
        if self.context.core_topic:
            parts.append(f"ä¸»é¢˜ï¼š{self.context.core_topic}")
        if self.context.domain:
            parts.append(f"é¢†åŸŸï¼š{self.context.domain}")
        if self.context.key_concepts:
            concepts = ", ".join(self.context.key_concepts[:10])
            parts.append(f"å…³é”®æ¦‚å¿µï¼š{concepts}")
        parts.append("")
        
        # æœ¯è¯­è¡¨
        if self.context.terminology:
            parts.append("## æœ¯è¯­è¡¨ï¼ˆå¿…é¡»ç»Ÿä¸€ä½¿ç”¨ï¼‰")
            for eng, chn in list(self.context.terminology.items())[:20]:
                parts.append(f"- {eng} = {chn}")
            parts.append("")
        
        # å‰æ–‡æ‘˜è¦
        if previous_context:
            parts.append("## å‰æ–‡æ‘˜è¦")
            parts.append(previous_context)
            parts.append("")
        
        # å¾…çŸ«æ­£çš„å­—å¹•
        parts.append("## å¾…çŸ«æ­£çš„å­—å¹•")
        for i, item in enumerate(chunk.items):
            parts.append(f"[{i + 1}] {item.text}")
        parts.append("")
        
        # è¾“å‡ºè¦æ±‚
        parts.append("è¯·è¾“å‡ºçŸ«æ­£åçš„å­—å¹•ï¼Œæ ¼å¼ä¸º [åºå·] çŸ«æ­£åçš„ç¿»è¯‘")
        
        return "\n".join(parts)
    
    def parse_response(
        self,
        response: str,
        original_items: List[RefinedSRTItem]
    ) -> List[RefinedSRTItem]:
        """
        è§£æ LLM å“åº”ï¼Œæå–çŸ«æ­£åçš„ç¿»è¯‘
        
        Args:
            response: LLM å“åº”æ–‡æœ¬
            original_items: åŸå§‹æ¡ç›®åˆ—è¡¨
            
        Returns:
            çŸ«æ­£åçš„ SRT æ¡ç›®åˆ—è¡¨
        """
        # è§£æå“åº”ä¸­çš„ [åºå·] ç¿»è¯‘ æ ¼å¼
        pattern = re.compile(r'\[(\d+)\]\s*(.+?)(?=\n\[|\n*$)', re.DOTALL)
        matches = pattern.findall(response)
        
        # åˆ›å»ºç´¢å¼•åˆ°ç¿»è¯‘çš„æ˜ å°„
        translations: Dict[int, str] = {}
        for idx_str, text in matches:
            idx = int(idx_str) - 1  # è½¬æ¢ä¸º 0-based ç´¢å¼•
            text = text.strip()
            # æ¸…ç† Markdown æ ¼å¼
            text = self._clean_markdown(text)
            if text:
                translations[idx] = text
        
        # åˆ›å»ºç»“æœåˆ—è¡¨
        result: List[RefinedSRTItem] = []
        for i, item in enumerate(original_items):
            if i in translations:
                refined = RefinedSRTItem(
                    start_ms=item.start_ms,
                    end_ms=item.end_ms,
                    text=translations[i],
                    speaker=item.speaker,
                    is_refined=True,
                    original_text=item.original_text or item.text
                )
            else:
                # æ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„ç¿»è¯‘ï¼Œä¿æŒåŸæ ·
                refined = RefinedSRTItem(
                    start_ms=item.start_ms,
                    end_ms=item.end_ms,
                    text=item.text,
                    speaker=item.speaker,
                    is_refined=False,
                    original_text=item.original_text or item.text
                )
            result.append(refined)
        
        return result
    
    def _call_llm(self, prompt: str) -> Optional[str]:
        """
        è°ƒç”¨ LLM API
        
        Args:
            prompt: ç”¨æˆ·æç¤º
            
        Returns:
            LLM å“åº”æ–‡æœ¬ï¼Œå¤±è´¥è¿”å› None
        """
        messages = [
            {"role": "system", "content": self.SYSTEM_PROMPT},
            {"role": "user", "content": prompt}
        ]
        
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.3,
            "max_tokens": 4096
        }
        
        data = json.dumps(payload).encode("utf-8")
        
        last_error = None
        for attempt in range(self.MAX_RETRIES):
            try:
                req = urllib.request.Request(
                    self.base_url,
                    data=data,
                    headers={
                        "Content-Type": "application/json",
                        "Authorization": f"Bearer {self.api_key}"
                    }
                )
                
                with urllib.request.urlopen(req, timeout=120) as resp:
                    body = resp.read().decode("utf-8")
                    obj = json.loads(body)
                    choices = obj.get("choices", [])
                    if choices:
                        message = choices[0].get("message", {})
                        return message.get("content")
                    return None
                    
            except (urllib.error.URLError, urllib.error.HTTPError, json.JSONDecodeError) as e:
                last_error = e
                if attempt < self.MAX_RETRIES - 1:
                    time.sleep(self.RETRY_DELAY)
        
        return None
    
    def _clean_markdown(self, text: str) -> str:
        """æ¸…ç† Markdown æ ¼å¼"""
        # ç§»é™¤ç²—ä½“
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'__([^_]+)__', r'\1', text)
        # ç§»é™¤æ–œä½“
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        text = re.sub(r'_([^_]+)_', r'\1', text)
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'`([^`]+)`', r'\1', text)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        text = re.sub(r'^[-*]\s+', '', text, flags=re.MULTILINE)
        return text.strip()
    
    def _generate_context_summary(self, items: List[RefinedSRTItem]) -> str:
        """ç”Ÿæˆä¸Šä¸‹æ–‡æ‘˜è¦"""
        if not items:
            return ""
        
        # å–æœ€åå‡ æ¡ä½œä¸ºæ‘˜è¦
        last_items = items[-3:]
        texts = [item.text for item in last_items]
        return " ".join(texts)[:200]


# =============================================================================
# Localization Reviewerï¼ˆæœ¬åœ°åŒ–å®¡æŸ¥å™¨ï¼‰
# =============================================================================


class LocalizationReviewer:
    """
    å®¡æŸ¥ç¿»è¯‘çš„ä¸­å›½äººå¯ç†è§£æ€§
    
    æ£€æŸ¥é¡¹ï¼š
    1. ä¸è‡ªç„¶çš„ç›´è¯‘
    2. æœªè§£é‡Šçš„ä¸“ä¸šæœ¯è¯­
    3. è¿‡é•¿çš„å¥å­
    4. ä¸å¸¸ç”¨çš„è¡¨è¾¾æ–¹å¼
    5. ä¿ç•™çš„è‹±æ–‡æ˜¯å¦å¿…è¦
    """
    
    # TTS å­—ç¬¦é•¿åº¦é™åˆ¶ï¼ˆDoubao TTSï¼‰
    MAX_CHAR_LENGTH = 75
    
    # å¸¸è§çš„ä¸è‡ªç„¶ç›´è¯‘æ¨¡å¼ï¼ˆéœ€è¦æ›´é•¿çš„ä¸Šä¸‹æ–‡æ‰èƒ½åˆ¤æ–­ï¼‰
    LITERAL_TRANSLATION_PATTERNS = [
        (r'åœ¨\s*\d+\s*çš„æœ«å°¾', 'å¥æœ«ä½ç½®è¡¨è¾¾ä¸è‡ªç„¶'),
        (r'å®ƒæ˜¯\s*ä¸€ä¸ª\s*éå¸¸\s*\w+çš„', 'å¯èƒ½æ˜¯ç›´è¯‘çš„ "It is a very..."'),
        (r'è¿™æ˜¯\s*ä¸€ä¸ª\s*éå¸¸\s*\w+çš„', 'å¯èƒ½æ˜¯ç›´è¯‘çš„ "This is a very..."'),
        (r'æˆ‘è®¤ä¸ºè¿™å®é™…ä¸Š', 'å¯èƒ½æ˜¯ç›´è¯‘çš„ "I think this actually..."'),
    ]
    
    def __init__(self, llm_client: Optional[Any] = None):
        """
        åˆå§‹åŒ–æœ¬åœ°åŒ–å®¡æŸ¥å™¨
        
        Args:
            llm_client: å¯é€‰çš„ LLM å®¢æˆ·ç«¯ï¼Œç”¨äºæ›´æ™ºèƒ½çš„å®¡æŸ¥
        """
        self.llm = llm_client
    
    def review(self, items: List[RefinedSRTItem]) -> List[LocalizationIssue]:
        """
        å®¡æŸ¥ç¿»è¯‘çš„æœ¬åœ°åŒ–è´¨é‡
        
        Args:
            items: SRT æ¡ç›®åˆ—è¡¨
            
        Returns:
            æœ¬åœ°åŒ–é—®é¢˜åˆ—è¡¨
        """
        issues: List[LocalizationIssue] = []
        
        for i, item in enumerate(items):
            # æ£€æŸ¥å¥å­é•¿åº¦
            length_issue = self.check_sentence_length(item.text)
            if length_issue:
                issues.append(LocalizationIssue(
                    index=i,
                    issue_type="sentence_too_long",
                    original=item.text,
                    suggestion=length_issue,
                    severity=IssueSeverity.ERROR
                ))
            
            # æ£€æŸ¥ç›´è¯‘
            literal_issue = self._check_literal_translation(item.text)
            if literal_issue:
                issues.append(LocalizationIssue(
                    index=i,
                    issue_type="literal_translation",
                    original=item.text,
                    suggestion=literal_issue,
                    severity=IssueSeverity.WARNING
                ))
            
            # æ£€æŸ¥æœªè§£é‡Šçš„è‹±æ–‡æœ¯è¯­
            english_issue = self._check_unexplained_english(item.text)
            if english_issue:
                issues.append(LocalizationIssue(
                    index=i,
                    issue_type="unexplained_english",
                    original=item.text,
                    suggestion=english_issue,
                    severity=IssueSeverity.WARNING
                ))
        
        return issues
    
    def check_sentence_length(self, text: str) -> Optional[str]:
        """
        æ£€æŸ¥å¥å­é•¿åº¦æ˜¯å¦é€‚åˆ TTS
        
        Args:
            text: æ–‡æœ¬å†…å®¹
            
        Returns:
            é—®é¢˜æè¿°ï¼Œæ— é—®é¢˜è¿”å› None
        """
        if len(text) > self.MAX_CHAR_LENGTH:
            # å»ºè®®åˆ†å‰²ç‚¹
            split_points = self._find_split_points(text)
            if split_points:
                return f"æ–‡æœ¬é•¿åº¦ {len(text)} è¶…è¿‡ {self.MAX_CHAR_LENGTH} å­—ç¬¦é™åˆ¶ï¼Œå»ºè®®åœ¨ä½ç½® {split_points[0]} å¤„åˆ†å‰²"
            return f"æ–‡æœ¬é•¿åº¦ {len(text)} è¶…è¿‡ {self.MAX_CHAR_LENGTH} å­—ç¬¦é™åˆ¶ï¼Œå»ºè®®ç¼©çŸ­æˆ–åˆ†å‰²"
        return None
    
    def _find_split_points(self, text: str) -> List[int]:
        """æ‰¾åˆ°è‡ªç„¶çš„åˆ†å‰²ç‚¹"""
        split_chars = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼Œ', 'ï¼›', '.', '!', '?', ',', ';']
        points = []
        
        for i, char in enumerate(text):
            if char in split_chars and i > 20 and i < len(text) - 10:
                points.append(i + 1)
        
        # æŒ‰æ¥è¿‘ä¸­ç‚¹æ’åº
        mid = len(text) // 2
        points.sort(key=lambda x: abs(x - mid))
        
        return points[:3]
    
    def _check_literal_translation(self, text: str) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰ç”Ÿç¡¬çš„ç›´è¯‘"""
        for pattern, description in self.LITERAL_TRANSLATION_PATTERNS:
            if re.search(pattern, text):
                return description
        return None
    
    def _check_unexplained_english(self, text: str) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœªè§£é‡Šçš„è‹±æ–‡æœ¯è¯­"""
        # æŸ¥æ‰¾è¿ç»­çš„è‹±æ–‡å•è¯ï¼ˆè¶…è¿‡ 2 ä¸ªå•è¯ï¼‰
        english_phrases = re.findall(r'\b[A-Za-z]{3,}(?:\s+[A-Za-z]{3,}){1,}\b', text)
        
        if english_phrases:
            # è¿‡æ»¤æ‰å¸¸è§çš„å¯æ¥å—è‹±æ–‡
            acceptable = {'TED', 'OK', 'API', 'URL', 'AI', 'UI', 'UX', 'CEO', 'CTO'}
            unexpected = [p for p in english_phrases if p.upper() not in acceptable]
            
            if unexpected:
                return f"åŒ…å«æœªç¿»è¯‘çš„è‹±æ–‡çŸ­è¯­ï¼š{', '.join(unexpected[:3])}"
        
        return None
    
    def split_long_text(self, text: str) -> List[str]:
        """
        åˆ†å‰²è¿‡é•¿çš„æ–‡æœ¬
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            åˆ†å‰²åçš„æ–‡æœ¬åˆ—è¡¨
        """
        if len(text) <= self.MAX_CHAR_LENGTH:
            return [text]
        
        # æ‰¾åˆ°åˆ†å‰²ç‚¹
        split_points = self._find_split_points(text)
        
        if not split_points:
            # æ²¡æœ‰å¥½çš„åˆ†å‰²ç‚¹ï¼Œå¼ºåˆ¶åœ¨ä¸­é—´åˆ†å‰²
            mid = len(text) // 2
            return [text[:mid].strip(), text[mid:].strip()]
        
        # ä½¿ç”¨ç¬¬ä¸€ä¸ªåˆ†å‰²ç‚¹
        point = split_points[0]
        first_part = text[:point].strip()
        second_part = text[point:].strip()
        
        # é€’å½’å¤„ç†ä»ç„¶è¿‡é•¿çš„éƒ¨åˆ†
        result = []
        if len(first_part) > self.MAX_CHAR_LENGTH:
            result.extend(self.split_long_text(first_part))
        else:
            result.append(first_part)
        
        if len(second_part) > self.MAX_CHAR_LENGTH:
            result.extend(self.split_long_text(second_part))
        else:
            result.append(second_part)
        
        return result


# =============================================================================
# Output Generatorï¼ˆè¾“å‡ºç”Ÿæˆå™¨ï¼‰
# =============================================================================

import datetime
import yaml


class OutputGenerator:
    """
    ç”Ÿæˆæœ€ç»ˆè¾“å‡ºæ–‡ä»¶
    
    è¾“å‡ºæ ¼å¼ï¼š
    - SRT æ ¼å¼å­—å¹•
    - YAML æ ¼å¼æœ¯è¯­è¡¨
    - æ–‡æœ¬æ ¼å¼å¤„ç†æ—¥å¿—
    """
    
    DEFAULT_SPEAKER = "DEFAULT"
    SPEAKER_TAG_FORMAT = "[Speaker: {name}]"
    
    def generate_srt(
        self,
        items: List[RefinedSRTItem],
        include_speaker_tags: bool = False
    ) -> str:
        """
        ç”Ÿæˆ SRT æ ¼å¼è¾“å‡º
        
        Args:
            items: SRT æ¡ç›®åˆ—è¡¨
            include_speaker_tags: æ˜¯å¦åŒ…å«è¯´è¯äººæ ‡ç­¾
            
        Returns:
            SRT æ ¼å¼å­—ç¬¦ä¸²
        """
        import srt
        
        subs = []
        for i, item in enumerate(items, start=1):
            text = item.text
            
            # æ·»åŠ è¯´è¯äººæ ‡ç­¾
            if include_speaker_tags:
                speaker = item.speaker or self.DEFAULT_SPEAKER
                tag = self.SPEAKER_TAG_FORMAT.format(name=speaker)
                text = f"{tag} {text}"
            
            # æ¸…ç† Markdown æ ¼å¼
            text = self._clean_markdown(text)
            
            start_td = datetime.timedelta(milliseconds=item.start_ms)
            end_td = datetime.timedelta(milliseconds=item.end_ms)
            subs.append(srt.Subtitle(index=i, start=start_td, end=end_td, content=text))
        
        return srt.compose(subs)
    
    def generate_terminology_report(
        self,
        terminology: Dict[str, str]
    ) -> str:
        """
        ç”Ÿæˆæœ¯è¯­è¡¨æŠ¥å‘Šï¼ˆYAML æ ¼å¼ï¼‰
        
        Args:
            terminology: æœ¯è¯­æ˜ å°„å­—å…¸
            
        Returns:
            YAML æ ¼å¼å­—ç¬¦ä¸²
        """
        report = {
            "terminology": terminology,
            "count": len(terminology),
            "generated_at": datetime.datetime.now().isoformat()
        }
        
        return yaml.dump(report, allow_unicode=True, default_flow_style=False, sort_keys=False)
    
    def generate_processing_log(
        self,
        chunks: List[Chunk],
        issues: List[LocalizationIssue],
        total_items: int = 0,
        refined_count: int = 0
    ) -> str:
        """
        ç”Ÿæˆå¤„ç†æ—¥å¿—
        
        Args:
            chunks: å¤„ç†çš„ chunk åˆ—è¡¨
            issues: æœ¬åœ°åŒ–é—®é¢˜åˆ—è¡¨
            total_items: æ€»æ¡ç›®æ•°
            refined_count: å·²çŸ«æ­£æ¡ç›®æ•°
            
        Returns:
            æ—¥å¿—æ–‡æœ¬
        """
        lines = []
        lines.append("=" * 60)
        lines.append("GS è¯­ä¹‰çŸ«æ­£å¤„ç†æ—¥å¿—")
        lines.append("=" * 60)
        lines.append(f"ç”Ÿæˆæ—¶é—´: {datetime.datetime.now().isoformat()}")
        lines.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        lines.append("## å¤„ç†ç»Ÿè®¡")
        lines.append(f"- æ€»æ¡ç›®æ•°: {total_items}")
        lines.append(f"- å·²çŸ«æ­£æ¡ç›®æ•°: {refined_count}")
        lines.append(f"- Chunk æ•°é‡: {len(chunks)}")
        lines.append(f"- å‘ç°é—®é¢˜æ•°: {len(issues)}")
        lines.append("")
        
        # Chunk è¯¦æƒ…
        lines.append("## Chunk å¤„ç†è¯¦æƒ…")
        for chunk in chunks:
            lines.append(f"- Chunk {chunk.index}: {chunk.item_count} æ¡ç›®, "
                        f"{chunk.start_ms}ms - {chunk.end_ms}ms")
            if chunk.terminology_used:
                terms = list(chunk.terminology_used.items())[:5]
                terms_str = ", ".join(f"{k}={v}" for k, v in terms)
                lines.append(f"  æœ¯è¯­: {terms_str}")
        lines.append("")
        
        # é—®é¢˜åˆ—è¡¨
        if issues:
            lines.append("## æœ¬åœ°åŒ–é—®é¢˜")
            for issue in issues:
                severity = "âš ï¸" if issue.severity == IssueSeverity.WARNING else "âŒ"
                lines.append(f"{severity} [{issue.index}] {issue.issue_type}")
                lines.append(f"   åŸæ–‡: {issue.original[:50]}...")
                lines.append(f"   å»ºè®®: {issue.suggestion}")
            lines.append("")
        
        lines.append("=" * 60)
        return "\n".join(lines)
    
    def _clean_markdown(self, text: str) -> str:
        """æ¸…ç† Markdown æ ¼å¼"""
        # ç§»é™¤ç²—ä½“
        text = re.sub(r'\*\*([^*]+)\*\*', r'\1', text)
        text = re.sub(r'__([^_]+)__', r'\1', text)
        # ç§»é™¤æ–œä½“
        text = re.sub(r'\*([^*]+)\*', r'\1', text)
        text = re.sub(r'_([^_]+)_', r'\1', text)
        # ç§»é™¤ä»£ç å—
        text = re.sub(r'`([^`]+)`', r'\1', text)
        # ç§»é™¤æ ‡é¢˜æ ‡è®°
        text = re.sub(r'^#+\s*', '', text, flags=re.MULTILINE)
        # ç§»é™¤åˆ—è¡¨æ ‡è®°
        text = re.sub(r'^[-*]\s+', '', text, flags=re.MULTILINE)
        return text.strip()
    
    def write_outputs(
        self,
        output_dir: str,
        basename: str,
        items: List[RefinedSRTItem],
        terminology: Dict[str, str],
        chunks: List[Chunk],
        issues: List[LocalizationIssue],
        include_speaker_tags: bool = False
    ) -> Dict[str, str]:
        """
        å†™å…¥æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
        
        Args:
            output_dir: è¾“å‡ºç›®å½•
            basename: åŸºç¡€æ–‡ä»¶å
            items: SRT æ¡ç›®åˆ—è¡¨
            terminology: æœ¯è¯­æ˜ å°„
            chunks: chunk åˆ—è¡¨
            issues: é—®é¢˜åˆ—è¡¨
            include_speaker_tags: æ˜¯å¦åŒ…å«è¯´è¯äººæ ‡ç­¾
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„å­—å…¸
        """
        os.makedirs(output_dir, exist_ok=True)
        
        paths = {}
        
        # å†™å…¥ SRT
        srt_path = os.path.join(output_dir, f"{basename}.refined.audio.srt")
        srt_content = self.generate_srt(items, include_speaker_tags)
        with open(srt_path, "w", encoding="utf-8") as f:
            f.write(srt_content)
        paths["srt"] = srt_path
        
        # å†™å…¥æœ¯è¯­è¡¨
        if terminology:
            term_path = os.path.join(output_dir, f"{basename}.terminology.yaml")
            term_content = self.generate_terminology_report(terminology)
            with open(term_path, "w", encoding="utf-8") as f:
                f.write(term_content)
            paths["terminology"] = term_path
        
        # å†™å…¥å¤„ç†æ—¥å¿—
        log_path = os.path.join(output_dir, f"{basename}.processing.log")
        refined_count = sum(1 for item in items if item.is_refined)
        log_content = self.generate_processing_log(
            chunks, issues, len(items), refined_count
        )
        with open(log_path, "w", encoding="utf-8") as f:
            f.write(log_content)
        paths["log"] = log_path
        
        return paths


# =============================================================================
# SemanticRefinerï¼ˆä¸»æµç¨‹ï¼‰
# =============================================================================


class SemanticRefiner:
    """
    GS è¯­ä¹‰çŸ«æ­£ SRT ç¿»è¯‘çš„ä¸»æµç¨‹ç±»
    
    æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼š
    - ContextExtractor: æå– gs.md è¯­ä¹‰ä¸Šä¸‹æ–‡
    - ChunkManager: åˆ†æ®µç®¡ç†
    - LLMRefiner: LLM ç¿»è¯‘çŸ«æ­£
    - LocalizationReviewer: æœ¬åœ°åŒ–å®¡æŸ¥
    - OutputGenerator: è¾“å‡ºç”Ÿæˆ
    """
    
    def __init__(
        self,
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        model: Optional[str] = None,
        checkpoint_dir: Optional[str] = None
    ):
        """
        åˆå§‹åŒ–è¯­ä¹‰çŸ«æ­£å™¨
        
        Args:
            api_key: LLM API å¯†é’¥
            base_url: LLM API åŸºç¡€ URL
            model: LLM æ¨¡å‹åç§°
            checkpoint_dir: æ£€æŸ¥ç‚¹ä¿å­˜ç›®å½•
        """
        self.api_key = api_key
        self.base_url = base_url
        self.model = model
        self.checkpoint_dir = checkpoint_dir
        
        # ç»„ä»¶ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰
        self._context_extractor: Optional[ContextExtractor] = None
        self._chunk_manager: Optional[ChunkManager] = None
        self._llm_refiner: Optional[LLMRefiner] = None
        self._localization_reviewer: Optional[LocalizationReviewer] = None
        self._output_generator: Optional[OutputGenerator] = None
        
        # çŠ¶æ€
        self._semantic_context: Optional[SemanticContext] = None
        self._progress_callback: Optional[callable] = None
    
    def set_progress_callback(self, callback: callable) -> None:
        """è®¾ç½®è¿›åº¦å›è°ƒå‡½æ•°"""
        self._progress_callback = callback
    
    def _report_progress(self, progress: float, message: str) -> None:
        """æŠ¥å‘Šè¿›åº¦"""
        if self._progress_callback:
            self._progress_callback(progress, message)
        else:
            print(f"[{progress:.1f}%] {message}")
    
    def refine(
        self,
        gs_path: str,
        srt_path: str,
        output_path: Optional[str] = None,
        include_speaker_tags: bool = False
    ) -> RefineResult:
        """
        æ‰§è¡Œè¯­ä¹‰çŸ«æ­£ä¸»æµç¨‹
        
        Args:
            gs_path: gs.md æ–‡ä»¶è·¯å¾„
            srt_path: SRT æ–‡ä»¶è·¯å¾„
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            include_speaker_tags: æ˜¯å¦åŒ…å«è¯´è¯äººæ ‡ç­¾
            
        Returns:
            RefineResult å¯¹è±¡
        """
        from flexdub.core.subtitle import read_srt
        
        self._report_progress(0, "å¼€å§‹å¤„ç†...")
        
        # 1. è¯»å– gs.md
        self._report_progress(5, "è¯»å– gs.md...")
        if not os.path.exists(gs_path):
            raise FileNotFoundError(f"gs.md æ–‡ä»¶ä¸å­˜åœ¨: {gs_path}")
        
        with open(gs_path, "r", encoding="utf-8") as f:
            gs_content = f.read()
        
        # 2. æå–è¯­ä¹‰ä¸Šä¸‹æ–‡
        self._report_progress(10, "æå–è¯­ä¹‰ä¸Šä¸‹æ–‡...")
        self._context_extractor = ContextExtractor()
        self._semantic_context = self._context_extractor.extract(gs_content)
        
        self._report_progress(15, f"æå–åˆ° {len(self._semantic_context.terminology)} ä¸ªæœ¯è¯­, "
                             f"{len(self._semantic_context.speakers)} ä¸ªè¯´è¯äºº")
        
        # 3. è¯»å– SRT
        self._report_progress(20, "è¯»å– SRT æ–‡ä»¶...")
        srt_items = read_srt(srt_path)
        refined_items = srt_items_to_refined(srt_items)
        
        self._report_progress(25, f"è¯»å–åˆ° {len(refined_items)} æ¡å­—å¹•")
        
        # 4. åˆ›å»º chunks
        self._report_progress(30, "åˆ›å»ºå¤„ç†åˆ†æ®µ...")
        self._chunk_manager = ChunkManager(refined_items, self.checkpoint_dir)
        chunks = self._chunk_manager.create_chunks()
        
        # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
        start_chunk = self._chunk_manager.load_checkpoint()
        if start_chunk > 0:
            self._report_progress(35, f"ä»æ£€æŸ¥ç‚¹æ¢å¤ï¼Œè·³è¿‡å‰ {start_chunk} ä¸ª chunks")
        
        self._report_progress(35, f"å…± {len(chunks)} ä¸ª chunks")
        
        # 5. åˆå§‹åŒ– LLM Refiner
        self._llm_refiner = LLMRefiner(
            self._semantic_context,
            api_key=self.api_key,
            base_url=self.base_url,
            model=self.model
        )
        
        # 6. é€ chunk å¤„ç†
        for i, chunk in enumerate(chunks):
            if i < start_chunk:
                continue
            
            progress = 40 + (i / len(chunks)) * 40
            self._report_progress(progress, f"å¤„ç† Chunk {i + 1}/{len(chunks)}...")
            
            # è·å–ä¸Šä¸‹æ–‡
            previous_context = self._chunk_manager.get_context_for_chunk(i)
            
            # çŸ«æ­£
            refined_chunk_items, context_summary = self._llm_refiner.refine_chunk(
                chunk, previous_context
            )
            
            # æ›´æ–°ä¸Šä¸‹æ–‡
            self._chunk_manager.update_context(i, context_summary, {})
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            self._chunk_manager.save_checkpoint(i, refined_chunk_items)
        
        # 7. è·å–æ‰€æœ‰ç»“æœ
        self._report_progress(80, "åˆå¹¶å¤„ç†ç»“æœ...")
        all_items = self._chunk_manager.get_all_results()
        
        # 8. æœ¬åœ°åŒ–å®¡æŸ¥
        self._report_progress(85, "æ‰§è¡Œæœ¬åœ°åŒ–å®¡æŸ¥...")
        self._localization_reviewer = LocalizationReviewer()
        issues = self._localization_reviewer.review(all_items)
        
        self._report_progress(90, f"å‘ç° {len(issues)} ä¸ªæœ¬åœ°åŒ–é—®é¢˜")
        
        # 9. ç”Ÿæˆè¾“å‡º
        self._output_generator = OutputGenerator()
        
        if output_path:
            self._report_progress(95, "å†™å…¥è¾“å‡ºæ–‡ä»¶...")
            output_dir = os.path.dirname(output_path) or "."
            basename = os.path.splitext(os.path.basename(output_path))[0]
            
            self._output_generator.write_outputs(
                output_dir=output_dir,
                basename=basename,
                items=all_items,
                terminology=self._chunk_manager.terminology,
                chunks=chunks,
                issues=issues,
                include_speaker_tags=include_speaker_tags
            )
        
        # 10. æ„å»ºç»“æœ
        self._report_progress(100, "å¤„ç†å®Œæˆ!")
        
        log_content = self._output_generator.generate_processing_log(
            chunks, issues, len(all_items),
            sum(1 for item in all_items if item.is_refined)
        )
        
        return RefineResult(
            items=all_items,
            terminology_used=self._chunk_manager.terminology,
            issues=issues,
            processing_log=log_content
        )
    
    @property
    def semantic_context(self) -> Optional[SemanticContext]:
        """è·å–è¯­ä¹‰ä¸Šä¸‹æ–‡"""
        return self._semantic_context
    
    @property
    def progress(self) -> float:
        """è·å–å¤„ç†è¿›åº¦"""
        if self._chunk_manager:
            return self._chunk_manager.progress
        return 0.0
